# -*- coding: utf-8 -*-
"""
Unified RAG System with Training
–ï–¥–∏–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ RAG —Å runtime –¥–∏–∞–ª–æ–≥–∞–º–∏ –∏ –Ω–æ—á–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º
"""
import logging
import os
import sqlite3
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import json
import time
import requests
from bs4 import BeautifulSoup
import threading

logger = logging.getLogger(__name__)

# ChromaDB –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
try:
    import chromadb
    from chromadb.config import Settings
    from sentence_transformers import SentenceTransformer
    CHROMADB_AVAILABLE = True
except ImportError:
    logger.warning("ChromaDB or SentenceTransformers not available")
    CHROMADB_AVAILABLE = False


class UnifiedRAGSystem:
    """–ï–¥–∏–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ RAG: runtime + training"""
    
    def __init__(self, data_dir: str = "/app/data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True, parents=True)
        
        # SQLite –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        self.db_path = self.data_dir / "knowledge.db"
        self.init_sqlite()
        
        # ChromaDB –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        if CHROMADB_AVAILABLE:
            self.chroma_path = self.data_dir / "chroma_db"
            self.chroma_path.mkdir(exist_ok=True)
            self.init_chromadb()
            self.embedder = SentenceTransformer(
                'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
            )
        else:
            self.chroma_client = None
            self.collections = {}
            self.embedder = None
        
        # –°—Ç–∞—Ç—É—Å –æ–±—É—á–µ–Ω–∏—è
        self.training_active = False
        self.training_thread = None
        
        # –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        self.learning_sources = self._init_learning_sources()
        
        logger.info("‚úÖ Unified RAG System initialized")
    
    def _init_learning_sources(self) -> Dict[str, List[str]]:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è"""
        return {
            "wikipedia": [
                "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π_–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "–ú–∞—à–∏–Ω–Ω–æ–µ_–æ–±—É—á–µ–Ω–∏–µ", 
                "Python_(—è–∑—ã–∫_–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è)", "–ù–µ–π—Ä–æ–Ω–Ω–∞—è_—Å–µ—Ç—å",
                "–û–±—Ä–∞–±–æ—Ç–∫–∞_–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ_—è–∑—ã–∫–∞", "–ö–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ_–∑—Ä–µ–Ω–∏–µ",
                "–ê–ª–≥–æ—Ä–∏—Ç–º", "–°—Ç—Ä—É–∫—Ç—É—Ä–∞_–¥–∞–Ω–Ω—ã—Ö", "Git", "Docker",
                "FastAPI", "PostgreSQL", "Redis", "Kubernetes"
            ],
            "programming_topics": [
                "Python async", "Docker compose", "REST API design",
                "Database optimization", "Git workflows", "CI/CD",
                "Machine learning pipelines", "Neural networks",
                "Vector embeddings", "RAG systems", "LLM prompting"
            ],
            "russian_topics": [
                "–ë–∞—à–∫–æ—Ä—Ç–æ—Å—Ç–∞–Ω", "–£—Ñ–∞", "–†–æ—Å—Å–∏–π—Å–∫–∞—è_–§–µ–¥–µ—Ä–∞—Ü–∏—è",
                "–ò—Å—Ç–æ—Ä–∏—è_–†–æ—Å—Å–∏–∏", "–†—É—Å—Å–∫–∏–π_—è–∑—ã–∫", "–õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞_–†–æ—Å—Å–∏–∏"
            ],
            "science": [
                "–§–∏–∑–∏–∫–∞", "–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞", "–•–∏–º–∏—è", "–ë–∏–æ–ª–æ–≥–∏—è",
                "–ê—Å—Ç—Ä–æ–Ω–æ–º–∏—è", "–ö–≤–∞–Ω—Ç–æ–≤–∞—è_–º–µ—Ö–∞–Ω–∏–∫–∞", "–¢–µ–æ—Ä–∏—è_–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π"
            ]
        }
    
    def init_sqlite(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SQLite –±–∞–∑—ã"""
        conn = sqlite3.connect(str(self.db_path))
        cursor = conn.cursor()
        
        # –¢–∞–±–ª–∏—Ü–∞ –¥–∏–∞–ª–æ–≥–æ–≤ (runtime)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS dialogues (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_message TEXT NOT NULL,
                assistant_message TEXT NOT NULL,
                model_used VARCHAR(50),
                success_rating FLOAT DEFAULT 0.5,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                tags TEXT,
                collection VARCHAR(50) DEFAULT 'dialogues'
            )
        """)
        
        # –¢–∞–±–ª–∏—Ü–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö (training)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS training_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                content TEXT NOT NULL,
                source VARCHAR(100),
                topic VARCHAR(200),
                content_type VARCHAR(50),
                quality_score FLOAT DEFAULT 0.5,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                collection VARCHAR(50) DEFAULT 'training'
            )
        """)
        
        # –¢–∞–±–ª–∏—Ü–∞ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS training_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                started_at TIMESTAMP,
                finished_at TIMESTAMP,
                cycles_completed INTEGER,
                items_added INTEGER,
                success_rate FLOAT,
                status VARCHAR(50)
            )
        """)
        
        # –ò–Ω–¥–µ–∫—Å—ã
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_dialogues_rating ON dialogues(success_rating)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_training_topic ON training_data(topic)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_training_quality ON training_data(quality_score)")
        
        conn.commit()
        conn.close()
        
        logger.info(f"‚úÖ SQLite database initialized")
    
    def init_chromadb(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ChromaDB"""
        try:
            self.chroma_client = chromadb.PersistentClient(
                path=str(self.chroma_path),
                settings=Settings(anonymized_telemetry=False)
            )
            
            # –ö–æ–ª–ª–µ–∫—Ü–∏–∏
            self.collections = {
                "dialogues": self.chroma_client.get_or_create_collection("user_dialogues"),
                "training": self.chroma_client.get_or_create_collection("training_knowledge"),
                "code": self.chroma_client.get_or_create_collection("code_examples"),
                "solutions": self.chroma_client.get_or_create_collection("solutions"),
            }
            
            logger.info(f"‚úÖ ChromaDB initialized")
        except Exception as e:
            logger.error(f"ChromaDB init failed: {e}")
            self.chroma_client = None
            self.collections = {}
    
    # ==================== RUNTIME RAG ====================
    
    def add_dialogue(
        self,
        user_message: str,
        assistant_message: str,
        model_used: str = "unknown",
        success_rating: float = 0.5
    ) -> int:
        """–î–æ–±–∞–≤–∏—Ç—å –¥–∏–∞–ª–æ–≥ –≤ –±–∞–∑—É (runtime)"""
        conn = sqlite3.connect(str(self.db_path))
        cursor = conn.cursor()
        
        cursor.execute("""
            INSERT INTO dialogues 
            (user_message, assistant_message, model_used, success_rating)
            VALUES (?, ?, ?, ?)
        """, (user_message, assistant_message, model_used, success_rating))
        
        dialogue_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ ChromaDB
        if self.collections.get("dialogues") and self.embedder:
            try:
                text = f"USER: {user_message}\nASSISTANT: {assistant_message}"
                embedding = self.embedder.encode(text).tolist()
                
                self.collections["dialogues"].add(
                    embeddings=[embedding],
                    documents=[text],
                    metadatas=[{
                        "model": model_used,
                        "rating": success_rating,
                        "db_id": dialogue_id,
                        "type": "dialogue"
                    }],
                    ids=[f"dialogue_{dialogue_id}_{int(time.time())}"]
                )
            except Exception as e:
                logger.error(f"Failed to add to ChromaDB: {e}")
        
        logger.debug(f"Added dialogue #{dialogue_id}")
        return dialogue_id
    
    def search_knowledge(
        self,
        query: str,
        limit: int = 5,
        collection: str = None
    ) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
        if not self.collections or not self.embedder:
            return []
        
        try:
            query_embedding = self.embedder.encode(query).tolist()
            
            all_results = []
            
            # –ò—â–µ–º –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –∏–ª–∏ –≤–æ –≤—Å–µ—Ö
            collections_to_search = (
                [self.collections[collection]] if collection and collection in self.collections
                else list(self.collections.values())
            )
            
            for coll in collections_to_search:
                try:
                    results = coll.query(
                        query_embeddings=[query_embedding],
                        n_results=3
                    )
                    
                    if results and results['documents']:
                        for i, doc in enumerate(results['documents'][0]):
                            metadata = results['metadatas'][0][i] if results.get('metadatas') else {}
                            distance = results['distances'][0][i] if results.get('distances') else 0
                            
                            similarity = 1.0 - min(distance / 2.0, 1.0)
                            
                            all_results.append({
                                "text": doc,
                                "similarity": round(similarity, 3),
                                "collection": coll.name,
                                "metadata": metadata
                            })
                except Exception as e:
                    logger.debug(f"Skip collection: {e}")
                    continue
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ similarity
            all_results.sort(key=lambda x: x["similarity"], reverse=True)
            return all_results[:limit]
            
        except Exception as e:
            logger.error(f"Search failed: {e}")
            return []
    
    # ==================== TRAINING (WEB SCRAPING) ====================
    
    def scrape_wikipedia(self, topic: str) -> Optional[str]:
        """–ü–∞—Ä—Å–∏–Ω–≥ Wikipedia"""
        url = f"https://ru.wikipedia.org/wiki/{topic}"
        
        try:
            response = requests.get(url, timeout=15, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            for tag in soup(['script', 'style', 'nav', 'footer', 'header']):
                tag.decompose()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
            paragraphs = soup.find_all('p')
            text = '\n'.join([p.get_text().strip() for p in paragraphs if len(p.get_text()) > 100])
            
            return text[:5000] if text else None  # –ü–µ—Ä–≤—ã–µ 5000 —Å–∏–º–≤–æ–ª–æ–≤
            
        except Exception as e:
            logger.error(f"Wikipedia scrape error for {topic}: {e}")
            return None
    
    def scrape_programming_content(self, query: str) -> Optional[str]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç—Å–∫–∏—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ (—Å–∏–º—É–ª—è—Ü–∏—è)"""
        # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–¥–µ—Å—å –º–æ–∂–Ω–æ –ø–∞—Ä—Å–∏—Ç—å:
        # - GitHub README
        # - Real Python articles
        # - GeeksforGeeks
        # - MDN Web Docs
        
        # –ü–æ–∫–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏
        return f"Programming topic: {query}\n(Content would be scraped from real sources)"
    
    def add_training_content(
        self,
        content: str,
        source: str,
        topic: str,
        content_type: str = "article"
    ) -> int:
        """–î–æ–±–∞–≤–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç –≤ –±–∞–∑—É –æ–±—É—á–µ–Ω–∏—è"""
        conn = sqlite3.connect(str(self.db_path))
        cursor = conn.cursor()
        
        cursor.execute("""
            INSERT INTO training_data 
            (content, source, topic, content_type)
            VALUES (?, ?, ?, ?)
        """, (content, source, topic, content_type))
        
        content_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ ChromaDB –ø–æ —á–∞–Ω–∫–∞–º
        if self.collections.get("training") and self.embedder:
            try:
                # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
                chunk_size = 500
                chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]
                
                for i, chunk in enumerate(chunks):
                    if len(chunk.strip()) < 50:
                        continue
                    
                    embedding = self.embedder.encode(chunk).tolist()
                    
                    self.collections["training"].add(
                        embeddings=[embedding],
                        documents=[chunk],
                        metadatas=[{
                            "source": source,
                            "topic": topic,
                            "type": content_type,
                            "db_id": content_id,
                            "chunk_id": i
                        }],
                        ids=[f"train_{content_id}_{i}_{int(time.time())}"]
                    )
                
                logger.debug(f"Added {len(chunks)} chunks for {topic}")
                
            except Exception as e:
                logger.error(f"Failed to add training content: {e}")
        
        return content_id
    
    def training_cycle(self) -> bool:
        """–û–¥–∏–Ω —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è"""
        try:
            # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏ —Ç–µ–º—É
            import random
            category = random.choice(list(self.learning_sources.keys()))
            topics = self.learning_sources[category]
            topic = random.choice(topics)
            
            logger.info(f"üìö Training on: {category} / {topic}")
            
            # –ü–∞—Ä—Å–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç
            if category == "wikipedia":
                content = self.scrape_wikipedia(topic)
                source = "wikipedia"
            else:
                content = self.scrape_programming_content(topic)
                source = "web"
            
            if not content or len(content) < 100:
                logger.warning(f"Insufficient content for {topic}")
                return False
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –±–∞–∑—É
            self.add_training_content(
                content=content,
                source=source,
                topic=topic,
                content_type="article"
            )
            
            logger.info(f"‚úÖ Added {len(content)} chars for {topic}")
            return True
            
        except Exception as e:
            logger.error(f"Training cycle error: {e}")
            return False
    
    def run_night_training(
        self,
        hours: int = 8,
        cycles_per_hour: int = 4
    ):
        """–ù–æ—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"""
        self.training_active = True
        total_cycles = hours * cycles_per_hour
        interval = 3600 / cycles_per_hour
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Å—Ç–∞—Ä—Ç
        conn = sqlite3.connect(str(self.db_path))
        cursor = conn.cursor()
        cursor.execute("""
            INSERT INTO training_history (started_at, status)
            VALUES (?, 'running')
        """, (datetime.now(),))
        session_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        logger.info(f"üåô NIGHT TRAINING STARTED")
        logger.info(f"Duration: {hours}h ({total_cycles} cycles)")
        
        success_count = 0
        
        for i in range(total_cycles):
            if not self.training_active:
                logger.info("Training stopped by user")
                break
            
            logger.info(f"üîÑ Cycle {i+1}/{total_cycles}")
            
            if self.training_cycle():
                success_count += 1
            
            if i < total_cycles - 1:
                logger.info(f"üò¥ Sleeping {interval:.0f}s...\n")
                time.sleep(interval)
        
        # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ
        conn = sqlite3.connect(str(self.db_path))
        cursor = conn.cursor()
        cursor.execute("""
            UPDATE training_history 
            SET finished_at = ?, cycles_completed = ?, 
                items_added = ?, success_rate = ?, status = 'completed'
            WHERE id = ?
        """, (
            datetime.now(),
            total_cycles,
            success_count,
            success_count / max(total_cycles, 1),
            session_id
        ))
        conn.commit()
        conn.close()
        
        self.training_active = False
        
        stats = self.get_stats()
        logger.info(f"\nüéâ TRAINING COMPLETED!")
        logger.info(f"Success rate: {success_count}/{total_cycles}")
        logger.info(f"Total items in DB: {stats}")
    
    def start_training_thread(
        self,
        hours: int = 8,
        cycles_per_hour: int = 4
    ) -> bool:
        """–ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ"""
        if self.training_active:
            return False
        
        self.training_thread = threading.Thread(
            target=self.run_night_training,
            args=(hours, cycles_per_hour),
            daemon=True
        )
        self.training_thread.start()
        return True
    
    def stop_training(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è"""
        self.training_active = False
    
    # ==================== STATS ====================
    
    def get_stats(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"""
        conn = sqlite3.connect(str(self.db_path))
        cursor = conn.cursor()
        
        stats = {}
        
        # –ü–æ–¥—Å—á—ë—Ç –∑–∞–ø–∏—Å–µ–π
        cursor.execute("SELECT COUNT(*) FROM dialogues")
        stats["dialogues"] = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM training_data")
        stats["training_items"] = cursor.fetchone()[0]
        
        # –†–∞–∑–º–µ—Ä—ã
        db_size = self.db_path.stat().st_size / (1024 * 1024)
        stats["db_size_mb"] = round(db_size, 2)
        
        if self.chroma_path.exists():
            chroma_size = sum(
                f.stat().st_size for f in self.chroma_path.rglob('*') if f.is_file()
            ) / (1024 * 1024)
            stats["chroma_size_mb"] = round(chroma_size, 2)
        
        # –°—Ç–∞—Ç—É—Å –æ–±—É—á–µ–Ω–∏—è
        stats["training_active"] = self.training_active
        
        # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
        cursor.execute("""
            SELECT COUNT(*), SUM(items_added), AVG(success_rate)
            FROM training_history
            WHERE status = 'completed'
        """)
        row = cursor.fetchone()
        stats["training_sessions"] = row[0] if row[0] else 0
        stats["total_trained_items"] = row[1] if row[1] else 0
        stats["avg_success_rate"] = round(row[2], 3) if row[2] else 0
        
        conn.close()
        return stats


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
rag_system = UnifiedRAGSystem()
