# -*- coding: utf-8 -*-
"""
Model Hierarchy System
–°–∏—Å—Ç–µ–º–∞ –∏–µ—Ä–∞—Ä—Ö–∏–∏ –º–æ–¥–µ–ª–µ–π —Å –∞–≤—Ç–æ—ç—Å–∫–∞–ª–∞—Ü–∏–µ–π –∏ –æ–±–ª–∞—á–Ω—ã–º–∏ fallback
"""
import logging
import os
import time
import requests
from typing import Dict, Any, Optional
from datetime import datetime

logger = logging.getLogger(__name__)


class ModelHierarchy:
    """–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –º–æ–¥–µ–ª–µ–π —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —ç—Å–∫–∞–ª–∞—Ü–∏–µ–π"""
    
    def __init__(self, ollama_base_url: str = "http://host.docker.internal:11434"):
        self.ollama_base_url = ollama_base_url
        
        # –ò–µ—Ä–∞—Ä—Ö–∏—è –º–æ–¥–µ–ª–µ–π –æ—Ç –ø—Ä–æ—Å—Ç—ã—Ö –∫ —Å–ª–æ–∂–Ω—ã–º
        self.models = {
            "junior": {
                "name": "granite-code:3b",
                "description": "–ë—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –∑–∞–¥–∞—á",
                "max_retries": 2,
                "timeout": 30
            },
            "middle": {
                "name": "qwen2.5-coder:7b",
                "description": "–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∫–æ–¥–∞",
                "max_retries": 2,
                "timeout": 60
            },
            "senior": {
                "name": "qwen2.5:7b-instruct-q5_K_M",
                "description": "–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á",
                "max_retries": 3,
                "timeout": 90
            },
            "expert": {
                "name": "hf.co/sizzlebop/Toucan-Qwen2.5-7B:latest",
                "description": "–≠–∫—Å–ø–µ—Ä—Ç–Ω–∞—è –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å",
                "max_retries": 3,
                "timeout": 120
            }
        }
        
        self.stats = {
            "total_calls": 0,
            "escalations": 0,
            "cloud_calls": 0,
            "by_level": {level: 0 for level in self.models.keys()}
        }
    
    def call_model(
        self,
        prompt: str,
        level: str = "junior",
        temperature: float = 0.7,
        max_tokens: int = 2048
    ) -> Dict[str, Any]:
        """
        –í—ã–∑–æ–≤ –º–æ–¥–µ–ª–∏ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è
        
        Args:
            prompt: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
            level: –£—Ä–æ–≤–µ–Ω—å –º–æ–¥–µ–ª–∏ (junior, middle, senior, expert)
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (0.0-1.0)
            max_tokens: –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ
        """
        if level not in self.models:
            return {
                "success": False,
                "error": f"Unknown level: {level}",
                "available_levels": list(self.models.keys())
            }
        
        model_config = self.models[level]
        model_name = model_config["name"]
        
        self.stats["total_calls"] += 1
        self.stats["by_level"][level] += 1
        
        logger.info(f"ü§ñ Calling {level} model: {model_name}")
        
        start_time = time.time()
        
        try:
            response = requests.post(
                f"{self.ollama_base_url}/api/generate",
                json={
                    "model": model_name,
                    "prompt": prompt,
                    "temperature": temperature,
                    "stream": False,
                    "options": {
                        "num_predict": max_tokens
                    }
                },
                timeout=model_config["timeout"]
            )
            
            response.raise_for_status()
            result = response.json()
            
            elapsed = time.time() - start_time
            
            logger.info(f"‚úÖ {level} model responded in {elapsed:.2f}s")
            
            return {
                "success": True,
                "response": result.get("response", ""),
                "model": model_name,
                "level": level,
                "tokens": result.get("eval_count", 0),
                "elapsed": elapsed
            }
            
        except requests.exceptions.Timeout:
            logger.error(f"‚è±Ô∏è {level} model timed out")
            return {
                "success": False,
                "error": f"Timeout after {model_config['timeout']}s",
                "model": model_name,
                "level": level
            }
            
        except Exception as e:
            logger.error(f"‚ùå {level} model error: {e}")
            return {
                "success": False,
                "error": str(e),
                "model": model_name,
                "level": level
            }
    
    def escalate(
        self,
        prompt: str,
        current_level: str = "junior",
        reason: str = None
    ) -> Dict[str, Any]:
        """
        –≠—Å–∫–∞–ª–∞—Ü–∏—è –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π —É—Ä–æ–≤–µ–Ω—å –º–æ–¥–µ–ª–∏
        
        Args:
            prompt: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
            current_level: –¢–µ–∫—É—â–∏–π —É—Ä–æ–≤–µ–Ω—å
            reason: –ü—Ä–∏—á–∏–Ω–∞ —ç—Å–∫–∞–ª–∞—Ü–∏–∏
        """
        level_order = ["junior", "middle", "senior", "expert", "cloud"]
        
        if current_level not in level_order:
            current_level = "junior"
        
        current_index = level_order.index(current_level)
        
        if current_index >= len(level_order) - 1:
            logger.warning("üîù Already at highest level, calling cloud expert")
            return call_cloud_expert(prompt)
        
        next_level = level_order[current_index + 1]
        
        logger.warning(f"üîº Escalating from {current_level} to {next_level}")
        if reason:
            logger.info(f"   Reason: {reason}")
        
        self.stats["escalations"] += 1
        
        if next_level == "cloud":
            return call_cloud_expert(prompt)
        
        result = self.call_model(prompt, level=next_level, temperature=0.3)
        result["escalated"] = True
        result["from_level"] = current_level
        
        return result
    
    def get_stats(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π"""
        return {
            **self.stats,
            "cloud_percentage": round(
                (self.stats["cloud_calls"] / max(self.stats["total_calls"], 1)) * 100, 2
            ),
            "escalation_rate": round(
                (self.stats["escalations"] / max(self.stats["total_calls"], 1)) * 100, 2
            )
        }


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
model_hierarchy = ModelHierarchy()


# ==================== CLOUD EXPERTS ====================

def call_cloud_expert(prompt: str) -> Dict[str, Any]:
    """–í—ã–∑–æ–≤ –æ–±–ª–∞—á–Ω–æ–≥–æ —ç–∫—Å–ø–µ—Ä—Ç–∞ —Å fallback –Ω–∞ Gemini –∏ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏"""
    import os
    
    # –ü—Ä–æ–±—É–µ–º Groq (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1)
    groq_key = os.getenv("GROQ_API_KEY")
    
    if groq_key:
        logger.warning("""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ò–°–ü–û–õ–¨–ó–£–ï–¢–°–Ø GROQ API                         ‚ïë
‚ïë                                                              ‚ïë
‚ïë  –î–ª—è —Ä–∞–±–æ—Ç—ã –∏–∑ –†–æ—Å—Å–∏–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º VPN!                         ‚ïë
‚ïë  –ú–æ–¥–µ–ª—å: Llama 3.3 70B (Groq)                                ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
""")
        
        try:
            from groq import Groq
            
            client = Groq(api_key=groq_key)
            
            response = client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=4096,
                timeout=30
            )
            
            logger.info(f"‚úÖ Groq API —É—Å–ø–µ—à–Ω–æ –≤—ã–∑–≤–∞–Ω")
            model_hierarchy.stats["cloud_calls"] += 1
            
            return {
                "success": True,
                "response": response.choices[0].message.content,
                "model": "llama-3.3-70b (Groq)",
                "cloud_used": True,
                "tokens": response.usage.total_tokens
            }
            
        except Exception as e:
            logger.error(f"‚ùå Groq API failed: {e}")
            logger.error("üîç –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
            logger.error("   1. VPN –Ω–µ –≤–∫–ª—é—á–µ–Ω")
            logger.error("   2. –ü—Ä–æ–±–ª–µ–º—ã —Å —Å–µ—Ç—å—é")
            logger.error("   3. –ù–µ–≤–µ—Ä–Ω—ã–π API –∫–ª—é—á")
            logger.info("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ Gemini...")
    
    # Fallback –Ω–∞ Gemini (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2)
    return call_gemini_expert(prompt)


def call_gemini_expert(prompt: str) -> Dict[str, Any]:
    """–í—ã–∑–æ–≤ Google Gemini API —Å fallback –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏"""
    import os
    
    gemini_key = os.getenv("GOOGLE_GEMINI_KEY")
    
    if not gemini_key:
        logger.info("‚ÑπÔ∏è  GOOGLE_GEMINI_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É—é –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏")
        return call_local_expert(prompt)
    
    logger.warning("""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ò–°–ü–û–õ–¨–ó–£–ï–¢–°–Ø GOOGLE GEMINI API                ‚ïë
‚ïë                                                              ‚ïë
‚ïë  –î–ª—è —Ä–∞–±–æ—Ç—ã –∏–∑ –†–æ—Å—Å–∏–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º VPN!                         ‚ïë
‚ïë  –ú–æ–¥–µ–ª—å: Gemini 2.0 Flash                                    ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
""")
    
    try:
        import google.generativeai as genai
        
        genai.configure(api_key=gemini_key)
        model = genai.GenerativeModel('gemini-2.0-flash-exp')
        
        response = model.generate_content(
            prompt,
            generation_config={
                'temperature': 0.3,
                'max_output_tokens': 4096,
            }
        )
        
        logger.info(f"‚úÖ Gemini API —É—Å–ø–µ—à–Ω–æ –≤—ã–∑–≤–∞–Ω")
        model_hierarchy.stats["cloud_calls"] += 1
        
        return {
            "success": True,
            "response": response.text,
            "model": "gemini-2.0-flash",
            "cloud_used": True
        }
        
    except Exception as e:
        logger.error(f"‚ùå Gemini API failed: {e}")
        logger.info("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—É—é —ç–∫—Å–ø–µ—Ä—Ç–Ω—É—é –º–æ–¥–µ–ª—å...")
        return call_local_expert(prompt)


def call_local_expert(prompt: str) -> Dict[str, Any]:
    """–í—ã–∑–æ–≤ —Å–∞–º–æ–π —É–º–Ω–æ–π –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ (–±–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞)"""
    logger.info("üè† –í—ã–∑–æ–≤ –ª–æ–∫–∞–ª—å–Ω–æ–π —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    
    result = model_hierarchy.call_model(
        prompt=prompt,
        level="expert",
        temperature=0.3
    )
    
    if result['success']:
        logger.info(f"‚úÖ –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –æ—Ç–≤–µ—Ç–∏–ª–∞ ({result.get('tokens', 0)} —Ç–æ–∫–µ–Ω–æ–≤)")
    
    return {
        "success": result['success'],
        "response": result.get('response', ''),
        "model": "Toucan-Qwen2.5-7B (local)",
        "cloud_used": False,
        "tokens": result.get('tokens', 0)
    }