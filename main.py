from dotenv import load_dotenv
import os
from pathlib import Path

import asyncio
import aiofiles
from gigachat_api import GigaChatAPI
import logging
import requests
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
import hashlib
import json
import time
import re
import os
import sys
from pathlib import Path
from datetime import datetime, timedelta
from fastapi import FastAPI, File, UploadFile, HTTPException, Request, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse, JSONResponse, StreamingResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
import GPUtil
import psutil
import subprocess
import yt_dlp
import sqlite3
import threading
from queue import Queue
import ast
import urllib.parse
from duckduckgo_search import DDGS  # Fallback only
from bs4 import BeautifulSoup
import wikipedia
import base64
import pytz

# ==================== NEW SYSTEMS ====================
from test_system import test_system
from backup_system import backup_system
from model_hierarchy import model_hierarchy

# ==================== LOGGING SETUP (FIXED!) ====================
log_dir = Path("/app/logs")
log_dir.mkdir(parents=True, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_dir / 'ii_agent.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)


# ==================== PERFORMANCE MODULES ====================
from cache_manager import CacheManager
from async_scraper import AsyncScraper

logger = logging.getLogger(__name__)

# Load environment variables
env_path = Path(__file__).parent / '.env'
if env_path.exists():
    load_dotenv(env_path)
    logger.info(f'âœ… Loaded .env from {env_path}')
else:
    logger.warning('âš ï¸ .env file not found')

# Initialize performance modules
cache_manager = CacheManager(db_path='data/cache.db', ttl=3600)
async_scraper = AsyncScraper(timeout=10, max_concurrent=5)
logger.info('? Performance modules initialized')


# ĞšĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ñ‹
MODEL_TIMEOUT = 300
MAX_FILE_SIZE = 10 * 1024 * 1024
CHROMA_DB_PATH = "/app/data/chroma_db"
BACKUP_DIR = Path("/app/data/backups")
BACKUP_DIR.mkdir(parents=True, exist_ok=True)
OLLAMA_API_URL = "http://host.docker.internal:11434"

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° GPU
try:
    gpus = GPUtil.getGPUs()
    HAS_GPU = len(gpus) > 0
    if HAS_GPU:
        logger.info(f"GPU detected: {gpus[0].name}, VRAM: {gpus[0].memoryTotal}MB")
    else:
        logger.warning("GPU not detected")
except:
    HAS_GPU = False
    logger.warning("GPU detection failed")

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ChromaDB
try:
    chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
    embedder = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
    CHROMADB_AVAILABLE = True
    logger.info("âœ… ChromaDB and SentenceTransformer initialized")
except Exception as e:
    CHROMADB_AVAILABLE = False
    logger.warning(f"âš ï¸ ChromaDB not available: {e}")

# ==================== Ğ‘ĞĞ—Ğ Ğ”ĞĞĞĞ«Ğ¥ ====================
def init_db():
    """Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ SQLite Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"""
    conn = sqlite3.connect('/app/data/ii_agent.db')
    c = conn.cursor()
    
    c.execute('''CREATE TABLE IF NOT EXISTS conversations
                 (id INTEGER PRIMARY KEY AUTOINCREMENT,
                  user_id TEXT,
                  query TEXT,
                  response TEXT,
                  sources TEXT,
                  model_used TEXT,
                  timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                  rating INTEGER DEFAULT 0)''')
    
    c.execute('''CREATE TABLE IF NOT EXISTS knowledge_base
                 (id INTEGER PRIMARY KEY AUTOINCREMENT,
                  topic TEXT,
                  content TEXT,
                  source TEXT,
                  confidence REAL DEFAULT 0.5,
                  timestamp DATETIME DEFAULT CURRENT_TIMESTAMP)''')
    
    conn.commit()
    conn.close()

def save_conversation(query, response, sources, model_used, user_id="default"):
    """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ° Ğ² Ğ±Ğ°Ğ·Ñƒ"""
    conn = sqlite3.connect('/app/data/ii_agent.db')
    c = conn.cursor()
    c.execute("INSERT INTO conversations (user_id, query, response, sources, model_used) VALUES (?, ?, ?, ?, ?)",
              (user_id, query, response, json.dumps(sources), model_used))
    conn.commit()
    conversation_id = c.lastrowid
    conn.close()
    return conversation_id

def get_db_cursor():
    """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ĞºÑƒÑ€ÑĞ¾Ñ€ Ğ‘Ğ”"""
    conn = sqlite3.connect('/app/data/ii_agent.db')
    return conn.cursor()

def add_knowledge(topic, content, source, confidence=0.8):
    """Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ±Ğ°Ğ·Ñƒ"""
    conn = sqlite3.connect('/app/data/ii_agent.db')
    c = conn.cursor()
    c.execute("INSERT INTO knowledge_base (topic, content, source, confidence) VALUES (?, ?, ?, ?)",
              (topic, content, source, confidence))
    conn.commit()
    conn.close()

def search_knowledge_base(query):
    """ĞŸĞ¾Ğ¸ÑĞº Ğ² Ğ±Ğ°Ğ·Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹"""
    conn = sqlite3.connect('/app/data/ii_agent.db')
    c = conn.cursor()
    c.execute("SELECT topic, content, source, confidence FROM knowledge_base WHERE topic LIKE ? OR content LIKE ? ORDER BY confidence DESC LIMIT 5",
              (f"%{query}%", f"%{query}%"))
    results = c.fetchall()
    conn.close()
    return results

def get_conversation_history(user_id="default", limit=50):
    """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²"""
    conn = sqlite3.connect('/app/data/ii_agent.db')
    c = conn.cursor()
    c.execute("SELECT query, response, timestamp FROM conversations WHERE user_id=? ORDER BY timestamp DESC LIMIT ?",
              (user_id, limit))
    history = c.fetchall()
    conn.close()
    return history

def get_db_stats():
    """Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ‘Ğ”"""
    conn = sqlite3.connect('/app/data/ii_agent.db')
    c = conn.cursor()
    
    c.execute("SELECT COUNT(*) FROM conversations")
    total_conversations = c.fetchone()[0]
    
    c.execute("SELECT COUNT(*) FROM knowledge_base")
    total_knowledge = c.fetchone()[0]
    
    conn.close()
    
    return {
        "conversations": total_conversations,
        "knowledge_items": total_knowledge
    }

# Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ‘Ğ”
init_db()
# ==================== WEB ĞŸĞĞ˜Ğ¡Ğš ====================
def search_web(query, max_results=5):
    """ĞŸĞ¾Ğ¸ÑĞº Ñ‡ĞµÑ€ĞµĞ· Google Custom Search Engine"""
    try:
        google_key = os.getenv('GOOGLE_API_KEY')
        google_cx = os.getenv('GOOGLE_CSE_CX')
        
        if not google_key or not google_cx:
            logger.error('âŒ Google API keys not found in .env')
            return []
        
        url = 'https://www.googleapis.com/customsearch/v1'
        params = {
            'key': google_key,
            'cx': google_cx,
            'q': query,
            'num': max_results
        }
        
        # Check cache first
        cache_key = f"{query}_{max_results}"
        cached_results = cache_manager.get(cache_key, 'google_cse')
        if cached_results:
            logger.info(f'ğŸ¯ Cache HIT for Google CSE: {query[:50]}...')
            return cached_results

        logger.info(f'ğŸ” Google CSE search: {query}')
        response = requests.get(url, params=params, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            results = []
            for item in data.get('items', []):
                results.append({
                    'title': item.get('title', ''),
                    'url': item.get('link', ''),
                    'snippet': item.get('snippet', '')
                })
            # Save to cache
            if results:
                cache_manager.set(cache_key, results, 'google_cse', ttl=3600)
                logger.info(f'ğŸ’¾ Cached {len(results)} results')
            
            logger.info(f'âœ… Found {len(results)} results from Google CSE')
            return results
        else:
            logger.error(f'âŒ Google CSE error: {response.status_code}')
            return []
            
    except Exception as e:
        logger.error(f'âŒ Search error: {e}')
        return []


def get_weather(city='Ğ£Ñ„Ğ°'):
    """ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ° Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ñ‹ Ñ‡ĞµÑ€ĞµĞ· OpenWeatherMap API"""
    try:
        api_key = os.getenv('OPENWEATHER_API_KEY')
        if not api_key:
            logger.warning('OpenWeatherMap API key not found')
            return None
        
        url = f'http://api.openweathermap.org/data/2.5/weather'
        params = {
            'q': city,
            'appid': api_key,
            'units': 'metric',
            'lang': 'ru'
        }
        
        logger.info(f'ğŸŒ¤ï¸ OpenWeatherMap API: {city}')
        response = requests.get(url, params=params, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            weather_info = {
                'temperature': data['main']['temp'],
                'feels_like': data['main']['feels_like'],
                'description': data['weather'][0]['description'],
                'humidity': data['main']['humidity'],
                'wind_speed': data['wind']['speed'],
                'city': data['name']
            }
            logger.info(f'âœ… Weather data received for {city}')
            return weather_info
        else:
            logger.error(f'âŒ OpenWeatherMap error: {response.status_code}')
            return None
    except Exception as e:
        logger.error(f'âŒ Weather API error: {e}')
        return None


def scrape_url(url, max_length=1500):
    """
    Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° ÑĞ¾ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ñ Ñ„Ğ¾ĞºÑƒÑĞ¾Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚
    
    Args:
        url: URL ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
        max_length: ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ»Ğ¸Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ° (Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ 1500)
    
    Returns:
        str: Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ»Ğ¸ Ğ¿ÑƒÑÑ‚Ğ°Ñ ÑÑ‚Ñ€Ğ¾ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞµ
    """
    try:
        logger.info(f'ğŸŒ Scraping URL: {url}')
        
        response = requests.get(url, timeout=15, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8'
        })
        response.encoding = 'utf-8'
        
        if response.status_code != 200:
            logger.warning(f'âš ï¸ HTTP {response.status_code} for {url}')
            return ""
        
        soup = BeautifulSoup(response.content, 'lxml')
        
        # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ğ½ĞµĞ½ÑƒĞ¶Ğ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹
        for script in soup(['script', 'style', 'nav', 'footer', 'aside', 'header', 'iframe', 'noscript', 'form']):
            script.decompose()
        
        # Ğ˜Ñ‰ĞµĞ¼ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ (Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ½Ñ‹Ğµ ÑĞµĞ»ĞµĞºÑ‚Ğ¾Ñ€Ñ‹)
        content = None
        selectors = [
            'article', 
            'main', 
            '.article-body',
            '.article-content', 
            '.news-content', 
            '.post-content',
            '.entry-content',
            '[itemprop="articleBody"]'
        ]
        
        for selector in selectors:
            content = soup.select_one(selector)
            if content:
                logger.info(f'âœ… Found content with: {selector}')
                break
        
        if content:
            text = content.get_text(separator=' ', strip=True)
        else:
            # Fallback: Ğ²ÑĞµ Ğ¿Ğ°Ñ€Ğ°Ğ³Ñ€Ğ°Ñ„Ñ‹
            paragraphs = soup.find_all('p')
            text = ' '.join([p.get_text().strip() for p in paragraphs if len(p.get_text().strip()) > 30])
            logger.info(f'âœ… Extracted {len(paragraphs)} paragraphs as fallback')
        
        # ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ğ¾Ñ‚ Ğ»Ğ¸ÑˆĞ½Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ²
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = ' '.join(chunk for chunk in chunks if chunk)
        
        # ĞĞ±Ñ€ĞµĞ·Ğ°ĞµĞ¼ Ğ´Ğ¾ Ğ½ÑƒĞ¶Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹
        if len(text) > max_length:
            text = text[:max_length] + '...'
        
        logger.info(f'âœ… Scraped {len(text)} chars from {url}')
        return text
        
    except requests.Timeout:
        logger.error(f'â±ï¸ Timeout scraping {url}')
        return ""
    except Exception as e:
        logger.error(f'âŒ Scraping error for {url}: {str(e)[:100]}')
        return ""


def detect_query_type(query):
    """
    ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ñ‚Ğ¸Ğ¿ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°
    
    Args:
        query: Ğ¢ĞµĞºÑÑ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ
    
    Returns:
        str: Ğ¢Ğ¸Ğ¿ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° ('weather', 'news', 'tutorial', 'general')
    """
    query_lower = query.lower()
    
    # ĞŸĞ¾Ğ³Ğ¾Ğ´Ğ°
    weather_keywords = ['Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ğ°', 'Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€', 'Ğ³Ñ€Ğ°Ğ´ÑƒÑ', 'Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·', 'ĞºĞ»Ğ¸Ğ¼Ğ°Ñ‚', 'weather', 'Ğ¾ÑĞ°Ğ´ĞºĞ¸', 'Ğ²ĞµÑ‚ĞµÑ€', 'Ğ´Ğ¾Ğ¶Ğ´', 'ÑĞ½ĞµĞ³']
    if any(kw in query_lower for kw in weather_keywords):
        logger.info('ğŸŒ¤ï¸ Query type: WEATHER')
        return 'weather'
    
    # ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸
    news_keywords = ['Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚', 'ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ', 'Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞ»Ğ¾', 'ÑĞ»ÑƒÑ‡Ğ¸Ğ»Ğ¾ÑÑŒ', 'news', 'ÑĞµĞ³Ğ¾Ğ´Ğ½Ñ', 'Ğ²Ñ‡ĞµÑ€Ğ°', 'Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ğµ']
    if any(kw in query_lower for kw in news_keywords):
        logger.info('ğŸ“° Query type: NEWS')
        return 'news'
    
    # Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸/Ğ³Ğ°Ğ¹Ğ´Ñ‹
    tutorial_keywords = ['ĞºĞ°Ğº ', 'Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸', 'ÑĞ¿Ğ¾ÑĞ¾Ğ±', 'Ğ¼ĞµÑ‚Ğ¾Ğ´', 'Ğ³Ğ°Ğ¹Ğ´', 'tutorial', 'Ğ¿Ğ¾ÑˆĞ°Ğ³', 'Ğ½Ğ°ÑƒÑ‡Ğ¸', 'Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸ ĞºĞ°Ğº']
    if any(kw in query_lower for kw in tutorial_keywords):
        logger.info('ğŸ“š Query type: TUTORIAL')
        return 'tutorial'
    
    # ĞĞ±Ñ‰Ğ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ (Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ)
    logger.info('ğŸ’¬ Query type: GENERAL')
    return 'general'

# ==================== OLLAMA Ğ’Ğ—ĞĞ˜ĞœĞĞ”Ğ•Ğ™Ğ¡Ğ¢Ğ’Ğ˜Ğ• ====================


def filter_chinese_characters(text: str) -> str:
    if not text:
        return text
    # Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ñ‹ Ğ´Ğ»Ñ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ñ… ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²  Ğ¿ÑƒĞ½ĞºÑ‚ÑƒĞ°Ñ†Ğ¸Ğ¸
    chinese_ranges = [
        (0x4E00, 0x9FFF),   # ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¸ĞµÑ€Ğ¾Ğ³Ğ»Ğ¸Ñ„Ñ‹
        (0x3400, 0x4DBF),   # Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ A
        (0x20000, 0x2A6DF), # Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ B
        (0x3000, 0x303F),   # Ğ¸Ñ‚Ğ°Ğ¹ÑĞºĞ°Ñ Ğ¿ÑƒĞ½ĞºÑ‚ÑƒĞ°Ñ†Ğ¸Ñ
        (0xFF00, 0xFFEF)    # Ğ¾Ğ»Ğ½Ğ¾ÑˆĞ¸Ñ€Ğ¸Ğ½Ğ½Ñ‹Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ‹
    ]
    filtered_chars = []
    for char in text:
        char_code = ord(char)
        is_chinese = any(start <= char_code <= end for start, end in chinese_ranges)
        if not is_chinese:
            filtered_chars.append(char)
    return ''.join(filtered_chars).strip()



def query_ai(prompt, provider='ollama', model='qwen2.5:7b-instruct-q5_K_M', temperature=0.7):
    """????????????? ????? AI: Ollama, Groq ??? GigaChat"""
    try:
        if provider == 'ollama':
            return query_ollama(prompt, model, temperature)
        
        elif provider == 'groq':
            from cloud_models import GroqAPI
            groq = GroqAPI()
            return groq.chat(prompt, model='llama-3.3-70b-versatile')
        
        elif provider == 'gigachat':
            from cloud_models import GigaChatAPI
            gigachat = GigaChatAPI()
            return gigachat.chat(prompt, model='GigaChat-Pro')
        
        else:
            logger.error(f'Unknown provider: {provider}')
            return query_ollama(prompt, model, temperature)
    
    except Exception as e:
        logger.error(f'AI query error ({provider}): {e}')
        return query_ollama(prompt, model, temperature)


def query_ollama(prompt, model="qwen2.5:7b-instruct-q5_K_M", temperature=0.7):
    """Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ Ğº Ollama LLM"""
    try:
        logger.info(f"ğŸ” Sending to Ollama: model={model}")
        
        response = requests.post(
            f"{OLLAMA_API_URL}/api/generate",
            json={
                "model": model,
                "prompt": prompt,
                "temperature": temperature,
                "stream": False
            },
            timeout=MODEL_TIMEOUT
        )
        
        if response.status_code == 200:
            return response.json().get("response", "")
        else:
            logger.error(f"Ollama API error: {response.status_code}")
            return None
            
    except Exception as e:
        logger.error(f"Ollama query error: {e}")
        return None

async def stream_ollama(prompt, model="qwen2.5:7b-instruct-q5_K_M", temperature=0.7):
    """Ğ¡Ñ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ¾Ñ‚ Ollama"""
    try:
        response = requests.post(
            f"{OLLAMA_API_URL}/api/generate",
            json={
                "model": model,
                "prompt": prompt,
                "temperature": temperature,
                "stream": True
            },
            stream=True,
            timeout=MODEL_TIMEOUT
        )
        
        for line in response.iter_lines():
            if line:
                data = json.loads(line)
                if "response" in data:
                    yield data["response"]
                    
    except Exception as e:
        logger.error(f"Ollama streaming error: {e}")
        yield f"Error: {str(e)}"

# ==================== ĞĞ”ĞœĞ˜ĞĞšĞ API ====================

app = FastAPI(
    title="II-Agent Pro API",
    description="Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ AI-Ğ°Ğ³ĞµĞ½Ñ‚ Ñ ÑĞ°Ğ¼Ğ¾Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸ĞµĞ¼",
    version="5.0"
)


@app.get("/api/admin/models")
async def get_ollama_models():
    """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ²ÑĞµÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ollama"""
    try:
        response = requests.get(f"{OLLAMA_API_URL}/api/tags", timeout=5)
        models_data = response.json()
        models = models_data.get("models", [])
        
        conn = sqlite3.connect('/app/data/ii_agent.db')
        c = conn.cursor()
        
        for model in models:
            model_name = model.get("name", "")
            c.execute("SELECT COUNT(*) FROM conversations WHERE model_used = ?", (model_name,))
            usage_count = c.fetchone()[0]
            model["usage_count"] = usage_count
            
        conn.close()
        
        return {"models": models, "status": "success"}
        
    except Exception as e:
        logger.error(f"Error getting models: {e}")
        return {"error": str(e), "models": []}

@app.post("/api/admin/models/pull")
async def pull_ollama_model(request: Request):
    """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ"""
    try:
        data = await request.json()
        model_name = data.get("model")
        
        if not model_name:
            raise HTTPException(status_code=400, detail="Model name required")
        
        response = requests.post(
            f"{OLLAMA_API_URL}/api/pull",
            json={"name": model_name},
            stream=True,
            timeout=3600
        )
        
        async def stream_progress():
            for line in response.iter_lines():
                if line:
                    yield f"data: {line.decode()}\n\n"
                    await asyncio.sleep(0.1)
        
        return StreamingResponse(stream_progress(), media_type="text/event-stream")
        
    except Exception as e:
        logger.error(f"Error pulling model: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.delete("/api/admin/models/{model_name}")
async def delete_ollama_model(model_name: str):
    """Ğ£Ğ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ"""
    try:
        response = requests.delete(
            f"{OLLAMA_API_URL}/api/delete",
            json={"name": model_name},
            timeout=30
        )
        
        if response.status_code == 200:
            return {"status": "success", "message": f"Model {model_name} deleted"}
        else:
            return {"status": "error", "message": f"Failed to delete: {response.text}"}
            
    except Exception as e:
        logger.error(f"Error deleting model: {e}")
        return {"error": str(e)}

@app.get("/api/admin/system/stats")
async def get_system_stats():
    """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½ÑƒÑ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ"""
    try:
        cpu_percent = psutil.cpu_percent(interval=1)
        cpu_count = psutil.cpu_count()
        
        ram = psutil.virtual_memory()
        ram_total = ram.total / (1024**3)
        ram_used = ram.used / (1024**3)
        ram_percent = ram.percent
        
        gpu_stats = []
        try:
            gpus = GPUtil.getGPUs()
            for gpu in gpus:
                gpu_stats.append({
                    "name": gpu.name,
                    "load": round(gpu.load * 100, 1),
                    "memory_used": gpu.memoryUsed,
                    "memory_total": gpu.memoryTotal,
                    "temperature": gpu.temperature
                })
        except:
            gpu_stats = []
        
        disk = psutil.disk_usage('/app/data')
        disk_total = disk.total / (1024**3)
        disk_used = disk.used / (1024**3)
        disk_percent = disk.percent
        
        return {
            "cpu": {"percent": round(cpu_percent, 1), "count": cpu_count},
            "ram": {"total_gb": round(ram_total, 2), "used_gb": round(ram_used, 2), "percent": round(ram_percent, 1)},
            "gpu": gpu_stats,
            "disk": {"total_gb": round(disk_total, 2), "used_gb": round(disk_used, 2), "percent": round(disk_percent, 1)},
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting system stats: {e}")
        return {"error": str(e)}

@app.get("/api/admin/logs")
async def get_logs(level: str = "INFO", limit: int = 100):
    """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ»Ğ¾Ğ³Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹"""
    try:
        log_file = Path("/app/logs/ii_agent.log")
        
        if not log_file.exists():
            return {"logs": [], "message": "Log file not found"}
        
        with open(log_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        if level != "ALL":
            filtered = [line for line in lines if level in line]
        else:
            filtered = lines
        
        return {"logs": filtered[-limit:], "total": len(filtered), "level": level}
        
    except Exception as e:
        logger.error(f"Error reading logs: {e}")
        return {"error": str(e), "logs": []}

@app.post("/api/admin/settings")
async def update_settings(request: Request):
    """ĞĞ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹"""
    try:
        settings = await request.json()
        config_file = Path("/app/data/settings.json")
        
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(settings, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Settings updated: {settings}")
        return {"status": "success", "settings": settings}
        
    except Exception as e:
        logger.error(f"Error updating settings: {e}")
        return {"error": str(e)}

@app.get("/api/admin/settings")
async def get_settings():
    """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸"""
    try:
        config_file = Path("/app/data/settings.json")
        
        if config_file.exists():
            with open(config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        default_settings = {
            "default_model": "qwen2.5:7b-instruct-q5_K_M",
            "temperature": 0.7,
            "max_tokens": 2000,
            "rag_top_k": 5,
            "enable_web_search": True,
            "enable_rag": True
        }
        
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(default_settings, f, indent=2)
        
        return default_settings
        
    except Exception as e:
        logger.error(f"Error loading settings: {e}")
        return {"error": str(e)}

# ==================== WEB SEARCH UNIFIED ====================
def web_search_unified(query: str, max_results: int = 5) -> list:
    """
    Unified web search with Google CSE primary and DuckDuckGo fallback.
    
    Args:
        query: Search query string
        max_results: Maximum number of results to return
        
    Returns:
        List of dicts with keys: url, title, snippet
    """
    results = []
    
    # Try Google Custom Search first
    google_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GOOGLE_CSE_API_KEY")
    google_cx = os.getenv("GOOGLE_CSE_CX") or os.getenv("GOOGLE_CSE_ID")
    
    if google_key and google_cx:
        try:
            logger.info(f"ğŸ” Google CSE search: {query}")
            url = "https://www.googleapis.com/customsearch/v1"
            params = {
                "key": google_key,
                "cx": google_cx,
                "q": query,
                "num": max_results
            }
            response = requests.get(url, params=params, timeout=10)
            response.raise_for_status()
            data = response.json()
            
            for item in data.get("items", []):
                results.append({
                    "url": item.get("link", ""),
                    "title": item.get("title", ""),
                    "snippet": item.get("snippet", "")
                })
            
            if results:
                logger.info(f"âœ… Google CSE: found {len(results)} results")
                return results
                
        except Exception as e:
            logger.warning(f"âš ï¸ Google CSE failed: {e}, falling back to DuckDuckGo")
    
    # Fallback to DuckDuckGo
    try:
        logger.info(f"ğŸ” DuckDuckGo search: {query}")
        ddg = DDGS()
        ddg_results = ddg.text(query, max_results=max_results)
        
        for item in ddg_results:
            results.append({
                "url": item.get("href", ""),
                "title": item.get("title", ""),
                "snippet": item.get("body", "")
            })
        
        logger.info(f"âœ… DuckDuckGo: found {len(results)} results")
        
    except Exception as e:
        logger.error(f"âŒ DuckDuckGo failed: {e}")
    
    return results


# ==================== FASTAPI Ğ˜ĞĞ˜Ğ¦Ğ˜ĞĞ›Ğ˜Ğ—ĞĞ¦Ğ˜Ğ¯ ====================

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== ĞĞ¡ĞĞĞ’ĞĞ«Ğ• ENDPOINTS ====================

@app.get("/")
async def root():
    """ĞšĞ¾Ñ€Ğ½ĞµĞ²Ğ¾Ğ¹ endpoint"""
    return {
        "service": "II-Agent Pro API",
        "version": "5.0",
        "status": "running",
        "features": ["chat", "rag", "web_search", "training", "backup", "admin"]
    }

@app.get("/health")
async def health_check():
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹"""
    try:
        ollama_response = requests.get(f"{OLLAMA_API_URL}/api/tags", timeout=5)
        ollama_status = "connected" if ollama_response.status_code == 200 else "disconnected"
        
        conn = sqlite3.connect('/app/data/ii_agent.db')
        conn.close()
        db_status = "healthy"
        
        chromadb_status = "available" if CHROMADB_AVAILABLE else "unavailable"
        
        return {
            "status": "healthy",
            "ollama": ollama_status,
            "database": db_status,
            "chromadb": chromadb_status,
            "gpu": "available" if HAS_GPU else "unavailable",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return {"status": "unhealthy", "error": str(e)}


@app.post("/ai-developer/generate")
async def generate_solution(request: dict):
    """ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸"""
    try:
        task = request.get('task', '')
        analysis = request.get('analysis', {})
        provider = request.get('provider', 'ollama')
        
        logger.info(f"ğŸ”„ Generating solution with {provider}...")
        
        # Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ ai_developer
        from ai_developer import AIdeveloper
        ai_dev = AIdev()
        
        # ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ
        solution = await ai_dev.generate_solution(task, analysis, provider)
        
        return {
            'success': True,
            'solution': solution
        }
    except Exception as e:
        logger.error(f"âŒ Solution generation error: {e}")
        return {
            'success': False,
            'error': str(e)
        }

@app.get("/stats")
async def get_stats():
    """ĞĞ±Ñ‰Ğ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹"""
    try:
        db_stats = get_db_stats()
        
        conn = sqlite3.connect('/app/data/ii_agent.db')
        c = conn.cursor()
        c.execute("SELECT model_used, COUNT(*) as count FROM conversations GROUP BY model_used")
        model_usage = {row[0]: row[1] for row in c.fetchall()}
        conn.close()
        
        uptime = time.time() - app.state.start_time if hasattr(app.state, 'start_time') else 0
        
        return {
            "total_conversations": db_stats["conversations"],
            "knowledge_items": db_stats["knowledge_items"],
            "model_usage": model_usage,
            "uptime_seconds": int(uptime),
            "chromadb_available": CHROMADB_AVAILABLE,
            "gpu_available": HAS_GPU
        }
    except Exception as e:
        logger.error(f"Stats error: {e}")
        return {"error": str(e)}

@app.get("/history")
async def get_history(limit: int = 50):
    """Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²"""
    try:
        history = get_conversation_history(limit=limit)
        return {
            "history": [{"query": h[0], "response": h[1], "timestamp": h[2]} for h in history],
            "count": len(history)
        }
    except Exception as e:
        logger.error(f"History error: {e}")
        return {"error": str(e)}

# ==================== Ğ§ĞĞ¢ ENDPOINTS ====================

@app.post("/chat")
async def chat(request: Request):
    """ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ñ‡Ğ°Ñ‚ endpoint"""
    try:
        data = await request.json()
        query = data.get("query", "")
        use_rag = data.get("use_rag", True)
        use_web = data.get("use_web", False)
        model = data.get("model", "qwen2.5:7b-instruct-q5_K_M")
        provider = data.get("provider", "ollama")
        
        if not query:
            raise HTTPException(status_code=400, detail="Query required")
        
        logger.info(f"Chat query: {query[:100]}...")
        
        context_parts = []
        sources = []
        
        if use_rag and CHROMADB_AVAILABLE:
            try:
                rag_results = search_knowledge_base(query)
                if rag_results:
                    context_parts.append("Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ±Ğ°Ğ·Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹:")
                    for topic, content, source, confidence in rag_results[:3]:
                        context_parts.append(f"- {content[:200]}")
                        sources.append({"type": "rag", "source": source, "confidence": confidence})
            except Exception as e:
                logger.error(f"RAG search error: {e}")
        
        if use_web:
            try:
                # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ñ‚Ğ¸Ğ¿ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°
                query_type = detect_query_type(query)
                
                # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ñ‹
                weather_keywords = ['Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ğ°', 'Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€', 'Ğ³Ñ€Ğ°Ğ´ÑƒÑ', 'weather', 'Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·', 'ĞºĞ»Ğ¸Ğ¼Ğ°Ñ‚', 'Ğ¾ÑĞ°Ğ´ĞºĞ¸', 'Ğ²ĞµÑ‚ĞµÑ€', 'Ğ´Ğ¾Ğ¶Ğ´', 'ÑĞ½ĞµĞ³']
                if any(keyword in query.lower() for keyword in weather_keywords):
                    # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ğ³Ğ¾Ñ€Ğ¾Ğ´ Ğ¸Ğ· Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° (Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ Ğ£Ñ„Ğ°)
                    city = 'Ğ£Ñ„Ğ°'
                    for word in ['Ğ² ', 'Ğ´Ğ»Ñ ', 'Ğ½Ğ° ']:
                        if word in query.lower():
                            parts = query.lower().split(word)
                            if len(parts) > 1:
                                city = parts[1].split()[0].capitalize()
                                break
                    
                    weather_data = get_weather(city)
                    if weather_data:
                        context_parts.append(f"\nĞ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ· Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ñ‹ Ğ´Ğ»Ñ {weather_data['city']}:")
                        context_parts.append(f"ğŸŒ¡ï¸ Ğ¢ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ°: {weather_data['temperature']}Â°C (Ğ¾Ñ‰ÑƒÑ‰Ğ°ĞµÑ‚ÑÑ ĞºĞ°Ğº {weather_data['feels_like']}Â°C)")
                        context_parts.append(f"â˜ï¸ Ğ¡Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ: {weather_data['description']}")
                        context_parts.append(f"ğŸ’§ Ğ’Ğ»Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ: {weather_data['humidity']}%")
                        context_parts.append(f"ğŸ’¨ Ğ’ĞµÑ‚ĞµÑ€: {weather_data['wind_speed']} Ğ¼/Ñ")
                        sources.append({"type": "weather", "city": city, "source": "OpenWeatherMap"})
                
                # ĞŸĞ¾Ğ¸ÑĞº Ğ² Google CSE
                web_results = search_web(query, max_results=5)
                
                if web_results:
                    # ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ‚Ğ¸Ğ¿Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°
                    if query_type == 'news':
                        # Ğ”Ğ›Ğ¯ ĞĞĞ’ĞĞ¡Ğ¢Ğ•Ğ™: ĞŸĞ°Ñ€ÑĞ¸Ğ¼ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ñ‹ ÑÑ‚Ğ°Ñ‚ĞµĞ¹
                        context_parts.append("\nğŸ“° ĞŸĞ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²:")
                        news_count = 0
                        
                        for result in web_results:
                            if news_count >= 3:  # ĞœĞ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ 3 Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸
                                break
                            
                            # ĞŸĞ°Ñ€ÑĞ¸Ğ¼ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸
                            article_text = scrape_url(result['url'], max_length=1200)
                            
                            if article_text and len(article_text) > 200:
                                # Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾ ÑĞ¿Ğ°Ñ€ÑĞ¸Ğ»Ğ¸ - Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚
                                context_parts.append(f"\nğŸ”¹ {result['title']}")
                                context_parts.append(f"{article_text}")
                                context_parts.append(f"[Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº: {result['url']}]")
                                sources.append({"type": "web", "title": result['title'], "url": result['url']})
                                news_count += 1
                            else:
                                # Fallback Ğ½Ğ° snippet ĞµÑĞ»Ğ¸ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ½Ğµ ÑƒĞ´Ğ°Ğ»ÑÑ
                                context_parts.append(f"\n- {result['title']}: {result['snippet'][:150]}")
                                sources.append({"type": "web", "title": result['title'], "url": result['url']})
                                news_count += 1
                                
                    elif query_type == 'tutorial':
                        # Ğ”Ğ›Ğ¯ Ğ˜ĞĞ¡Ğ¢Ğ Ğ£ĞšĞ¦Ğ˜Ğ™: ĞŸĞ°Ñ€ÑĞ¸Ğ¼ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹
                        context_parts.append("\nğŸ“š ĞŸĞ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸:")
                        
                        for result in web_results[:2]:  # ĞœĞ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ 2 Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°
                            article_text = scrape_url(result['url'], max_length=2000)
                            
                            if article_text and len(article_text) > 300:
                                context_parts.append(f"\nğŸ“– {result['title']}")
                                context_parts.append(f"{article_text}")
                                context_parts.append(f"[Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº: {result['url']}]")
                                sources.append({"type": "web", "title": result['title'], "url": result['url']})
                            else:
                                # Fallback Ğ½Ğ° snippet
                                context_parts.append(f"- {result['title']}: {result['snippet']}")
                                sources.append({"type": "web", "title": result['title'], "url": result['url']})
                    
                    else:
                        # Ğ”Ğ›Ğ¯ ĞĞ‘Ğ©Ğ˜Ğ¥ Ğ’ĞĞŸĞ ĞĞ¡ĞĞ’: Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ snippets Ğ¸Ğ· Google
                        context_parts.append("\nĞ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğ°:")
                        for result in web_results[:3]:
                            context_parts.append(f"- {result['title']}: {result['snippet'][:150]} [Ğ¡ÑÑ‹Ğ»ĞºĞ°: {result['url']}]")
                            sources.append({"type": "web", "title": result['title'], "url": result['url']})
                            
            except Exception as e:
                logger.error(f"Web search error: {e}")
        
        context = "\n".join(context_parts) if context_parts else ""
        
        full_prompt = f"""Ğ¢Ñ‹ - II-Agent Pro, Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ AI-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ….

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš ï¸ ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜ Ğ’ĞĞ–ĞĞ - Ğ¯Ğ—Ğ«Ğš ĞĞ¢Ğ’Ğ•Ğ¢Ğ (Ğ§Ğ˜Ğ¢ĞĞ™ Ğ’ĞĞ˜ĞœĞĞ¢Ğ•Ğ›Ğ¬ĞĞ!):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸš« ĞĞ‘Ğ¡ĞĞ›Ğ®Ğ¢ĞĞ Ğ—ĞĞŸĞ Ğ•Ğ©Ğ•ĞĞ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ:
   - ĞšĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº: ä¸­æ–‡, æ±‰è¯­, å¤©æ°”, é¢„æŠ¥, ç›¸å…³, ä¿¡æ¯, ç­‰ç­‰
   - ĞĞ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº: English, weather, forecast, information, etc.
   - Ğ›ÑĞ±Ñ‹Ğµ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¸!

âœ… Ğ•Ğ”Ğ˜ĞĞ¡Ğ¢Ğ’Ğ•ĞĞĞ«Ğ™ Ğ ĞĞ—Ğ Ğ•Ğ¨ĞĞĞĞ«Ğ™ Ğ¯Ğ—Ğ«Ğš: Ğ Ğ£Ğ¡Ğ¡ĞšĞ˜Ğ™ (ĞºĞ¸Ñ€Ğ¸Ğ»Ğ»Ğ¸Ñ†Ğ°)!

Ğ¡Ğ¢Ğ ĞĞ“Ğ˜Ğ• ĞŸĞ ĞĞ’Ğ˜Ğ›Ğ (ĞĞĞ Ğ£Ğ¨Ğ•ĞĞ˜Ğ• = ĞĞ¨Ğ˜Ğ‘ĞšĞ):
1. ĞŸĞ¸ÑˆĞ¸ Ğ¢ĞĞ›Ğ¬ĞšĞ Ñ€ÑƒÑÑĞºĞ¸Ğ¼Ğ¸ Ğ±ÑƒĞºĞ²Ğ°Ğ¼Ğ¸: Ğ°, Ğ±, Ğ², Ğ³, Ğ´, Ğµ, Ñ‘, Ğ¶, Ğ·, Ğ¸, Ğ¹, Ğº, Ğ», Ğ¼, Ğ½, Ğ¾, Ğ¿, Ñ€, Ñ, Ñ‚, Ñƒ, Ñ„, Ñ…, Ñ†, Ñ‡, Ñˆ, Ñ‰, ÑŠ, Ñ‹, ÑŒ, Ñ, Ñ, Ñ
2. Ğ—ĞĞŸĞ Ğ•Ğ©Ğ•ĞĞ« ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğµ Ğ¸ĞµÑ€Ğ¾Ğ³Ğ»Ğ¸Ñ„Ñ‹: ä¸­, æ–‡, å¤©, æ°”, é¢„, æŠ¥, ç›¸, å…³, ä¿¡, æ¯
3. Ğ—ĞĞŸĞ Ğ•Ğ©Ğ•ĞĞ Ğ»Ğ°Ñ‚Ğ¸Ğ½Ğ¸Ñ†Ğ° Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ (Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ° Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² URL)
4. Ğ•ÑĞ»Ğ¸ Ğ·Ğ°Ğ¼ĞµÑ‡Ğ°ĞµÑˆÑŒ Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ĞµÑˆÑŒ Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ - ĞĞ¡Ğ¢ĞĞĞĞ’Ğ˜ Ğ¡Ğ•Ğ‘Ğ¯!
5. ĞŸĞµÑ€ĞµÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ğ¹ ĞºĞ°Ğ¶Ğ´Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ´ Ñ‚ĞµĞ¼ ĞºĞ°Ğº ĞµĞ³Ğ¾ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ
6. Ğ­Ñ‚Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ´Ğ¾ Ğ¡ĞĞœĞĞ“Ğ ĞšĞĞĞ¦Ğ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°!
7. ĞĞ• ĞŸĞ˜Ğ¨Ğ˜ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğµ ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ğ½Ğ¸Ñ!

ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ ĞŸĞ•Ğ Ğ•Ğ” ĞĞ¢ĞŸĞ ĞĞ’ĞšĞĞ™:
- Ğ•ÑÑ‚ÑŒ Ğ»Ğ¸ Ğ² Ğ¼Ğ¾Ñ‘Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğµ Ğ¸ĞµÑ€Ğ¾Ğ³Ğ»Ğ¸Ñ„Ñ‹? â†’ Ğ£Ğ”ĞĞ›Ğ˜ Ğ˜Ğ¥!
- Ğ•ÑÑ‚ÑŒ Ğ»Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğµ ÑĞ»Ğ¾Ğ²Ğ° Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ? â†’ ĞŸĞ•Ğ Ğ•Ğ’Ğ•Ğ”Ğ˜ ĞĞ Ğ Ğ£Ğ¡Ğ¡ĞšĞ˜Ğ™!
- Ğ’ĞµÑÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ? â†’ Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ¢ĞĞ“Ğ”Ğ ĞĞ¢ĞŸĞ ĞĞ’Ğ›Ğ¯Ğ™!

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“‹ Ğ¡Ğ¢Ğ Ğ£ĞšĞ¢Ğ£Ğ Ğ Ğ˜ ĞšĞĞ§Ğ•Ğ¡Ğ¢Ğ’Ğ ĞĞ¢Ğ’Ğ•Ğ¢Ğ:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Ğ¤ĞĞ ĞœĞĞ¢ ĞĞ¢Ğ’Ğ•Ğ¢Ğ:
   âœ“ ĞĞ°Ñ‡Ğ¸Ğ½Ğ°Ğ¹ Ñ ĞºÑ€Ğ°Ñ‚ĞºĞ¾Ğ³Ğ¾ Ñ€ĞµĞ·ÑĞ¼Ğµ (1-2 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ)
   âœ“ Ğ”Ğ°Ğ²Ğ°Ğ¹ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ğ½ÑƒÑ‚Ñ‹Ğ¹, Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸
   âœ“ Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚: Ğ°Ğ±Ğ·Ğ°Ñ†Ñ‹, ÑĞ¿Ğ¸ÑĞºĞ¸, Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸
   âœ“ Ğ”Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚ĞµĞ¼ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°Ğ¹ Ğ½Ğ° Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ñ‹

2. ĞšĞĞ§Ğ•Ğ¡Ğ¢Ğ’Ğ Ğ˜ĞĞ¤ĞĞ ĞœĞĞ¦Ğ˜Ğ˜:
   âœ“ ĞŸÑ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ: Ñ†Ğ¸Ñ„Ñ€Ñ‹, Ğ´Ğ°Ñ‚Ñ‹, Ğ¸Ğ¼ĞµĞ½Ğ°, Ğ¼ĞµÑÑ‚Ğ°
   âœ“ Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°
   âœ“ Ğ§Ñ‘Ñ‚ĞºĞ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞ¹ Ñ„Ğ°ĞºÑ‚Ñ‹ Ğ¸ Ğ¼Ğ½ĞµĞ½Ğ¸Ñ
   âœ“ Ğ•ÑĞ»Ğ¸ Ğ½Ğµ ÑƒĞ²ĞµÑ€ĞµĞ½ - Ñ‚Ğ°Ğº Ğ¸ ÑĞºĞ°Ğ¶Ğ¸, Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ´ÑƒĞ¼Ñ‹Ğ²Ğ°Ğ¹

3. ĞŸĞĞ“ĞĞ”Ğ Ğ˜ Ğ Ğ•ĞĞ›Ğ¬ĞĞ«Ğ• Ğ”ĞĞĞĞ«Ğ•:
   âœ“ Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· OpenWeatherMap API
   âœ“ Ğ£ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ¹: Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñƒ, Ğ¾Ñ‰ÑƒÑ‰Ğ°ĞµÑ‚ÑÑ ĞºĞ°Ğº, Ğ²Ğ»Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ²ĞµÑ‚ĞµÑ€, Ğ¾ÑĞ°Ğ´ĞºĞ¸
   âœ“ Ğ”Ğ°Ğ²Ğ°Ğ¹ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ· Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹

4. ĞĞĞ’ĞĞ¡Ğ¢Ğ˜ Ğ˜ Ğ¡ĞĞ‘Ğ«Ğ¢Ğ˜Ğ¯:
   âœ“ ĞŸĞµÑ€ĞµÑ‡Ğ¸ÑĞ»ÑĞ¹ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ Ğ² Ñ…Ñ€Ğ¾Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞµ
   âœ“ Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ: ĞºÑ€Ğ°Ñ‚ĞºĞ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ 2-3 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ
   âœ“ Ğ£ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸, Ğ´Ğ°Ñ‚Ñ‹, ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑƒÑ‡Ğ°ÑÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²

5. Ğ¢Ğ•Ğ¥ĞĞ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ• Ğ’ĞĞŸĞ ĞĞ¡Ğ«:
   âœ“ Ğ”Ğ°Ğ²Ğ°Ğ¹ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ³Ğ´Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾
   âœ“ ĞĞ±ÑŠÑÑĞ½ÑĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ¼
   âœ“ ĞŸÑ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹

6. Ğ¡Ğ¢Ğ˜Ğ›Ğ¬ ĞĞ‘Ğ©Ğ•ĞĞ˜Ğ¯:
   âœ“ ĞŸÑ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹, Ğ½Ğ¾ Ğ´Ñ€ÑƒĞ¶ĞµĞ»ÑĞ±Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾Ğ½
   âœ“ Ğ˜Ğ·Ğ±ĞµĞ³Ğ°Ğ¹ Ğ¸Ğ·Ğ»Ğ¸ÑˆĞ½ĞµĞ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ "Ğ²Ğ¾Ğ´Ñ‹"
   âœ“ Ğ‘ÑƒĞ´ÑŒ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚ĞµĞ½ Ğ¸ Ğ¿Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ñƒ
   âœ“ ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞ¹ÑÑ Ğ¿Ğ¾Ğ´ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“š Ğ”ĞĞ¡Ğ¢Ğ£ĞŸĞĞĞ¯ Ğ˜ĞĞ¤ĞĞ ĞœĞĞ¦Ğ˜Ğ¯:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{context}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â“ Ğ’ĞĞŸĞ ĞĞ¡ ĞŸĞĞ›Ğ¬Ğ—ĞĞ’ĞĞ¢Ğ•Ğ›Ğ¯:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{query}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ’¬ Ğ¢Ğ’ĞĞ™ ĞĞ¢Ğ’Ğ•Ğ¢ (ĞŸĞĞœĞĞ˜: Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ Ğ£Ğ¡Ğ¡ĞšĞ˜Ğ™ Ğ¯Ğ—Ğ«Ğš Ğ”Ğ ĞšĞĞĞ¦Ğ!):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
        
        response = query_ai(full_prompt, provider=provider, model=model)
        if response:
            response = filter_chinese_characters(response)
        
        if response is None:
            raise HTTPException(status_code=500, detail="LLM query failed")
        
        conversation_id = save_conversation(query, response, sources, model)
        
        # ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ ÑÑÑ‹Ğ»ĞºĞ¸ Ğº Ğ¾Ñ‚Ğ²ĞµÑ‚Ñƒ
        if sources:
            links_section = "\n\n---\nğŸ“Œ **Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸:**\n"
            for src in sources:
                if src.get('type') == 'web':
                    links_section += f"- [{src['title']}]({src['url']})\n"
                elif src.get('type') == 'weather':
                    links_section += f"- ĞŸĞ¾Ğ³Ğ¾Ğ´Ğ°: OpenWeatherMap ({src['city']})\n"
            response += links_section
        
        return {
            "response": response,
            "sources": sources,
            "model": model,
            "conversation_id": conversation_id,
            "timestamp": datetime.now().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Chat error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ==================== RAG ENDPOINTS ====================

@app.post("/rag/add")
async def add_rag_item(request: Request):
    """Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚ Ğ² Ğ±Ğ°Ğ·Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹"""
    try:
        data = await request.json()
        topic = data.get("topic", "")
        content = data.get("content", "")
        source = data.get("source", "manual")
        confidence = data.get("confidence", 0.8)
        
        if not topic or not content:
            raise HTTPException(status_code=400, detail="Topic and content required")
        
        add_knowledge(topic, content, source, confidence)
        
        return {"status": "success", "message": "Knowledge added", "topic": topic}
        
    except Exception as e:
        logger.error(f"RAG add error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/rag/search")
async def search_rag(query: str, limit: int = 5):
    """ĞŸĞ¾Ğ¸ÑĞº Ğ² Ğ±Ğ°Ğ·Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹"""
    try:
        results = search_knowledge_base(query)
        
        return {
            "query": query,
            "results": [
                {"topic": r[0], "content": r[1], "source": r[2], "confidence": r[3]}
                for r in results[:limit]
            ],
            "count": len(results)
        }
        
    except Exception as e:
        logger.error(f"RAG search error: {e}")
        return {"error": str(e)}

@app.get("/rag/stats")
async def rag_stats():
    """Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° RAG"""
    try:
        conn = sqlite3.connect('/app/data/ii_agent.db')
        c = conn.cursor()
        
        c.execute("SELECT COUNT(*) FROM knowledge_base")
        total = c.fetchone()[0]
        
        c.execute("SELECT source, COUNT(*) as count FROM knowledge_base GROUP BY source")
        by_source = {row[0]: row[1] for row in c.fetchall()}
        
        c.execute("SELECT AVG(confidence) FROM knowledge_base")
        avg_confidence = c.fetchone()[0] or 0.0
        
        conn.close()
        
        return {
            "total_items": total,
            "by_source": by_source,
            "avg_confidence": round(avg_confidence, 2),
            "chromadb_available": CHROMADB_AVAILABLE
        }
        
    except Exception as e:
        logger.error(f"RAG stats error: {e}")
        return {"error": str(e)}

# ==================== TRAINING ENDPOINTS ====================

training_status = {
    "is_running": False,
    "progress": 0,
    "current_source": None,
    "items_processed": 0,
    "start_time": None
}

@app.post("/training/start")
async def start_training(request: Request):
    """Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ½Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ"""
    global training_status
    
    try:
        if training_status["is_running"]:
            return {"status": "error", "message": "Training already running"}
        
        data = await request.json()
        duration_hours = data.get("duration_hours", 4)
        cycles_per_hour = data.get("cycles_per_hour", 4)
        sources = data.get("sources", ["wikipedia", "github", "habr"])
        
        asyncio.create_task(run_training(duration_hours, cycles_per_hour, sources))
        
        return {
            "status": "started",
            "duration_hours": duration_hours,
            "cycles_per_hour": cycles_per_hour,
            "sources": sources
        }
        
    except Exception as e:
        logger.error(f"Training start error: {e}")
        return {"error": str(e)}

async def run_training(duration_hours, cycles_per_hour, sources):
    """Ğ¤Ğ¾Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ"""
    global training_status
    
    training_status["is_running"] = True
    training_status["progress"] = 0
    training_status["start_time"] = datetime.now().isoformat()
    
    try:
        total_cycles = duration_hours * cycles_per_hour
        
        for cycle in range(total_cycles):
            source = sources[cycle % len(sources)]
            training_status["current_source"] = source
            training_status["progress"] = int((cycle / total_cycles) * 100)
            
            if source == "wikipedia":
                await train_from_wikipedia()
            elif source == "github":
                await train_from_github()
            elif source == "habr":
                await train_from_habr()
            
            training_status["items_processed"] += 1
            
            await asyncio.sleep(60 * 60 / cycles_per_hour)
        
        training_status["progress"] = 100
        logger.info(f"Training completed: {training_status['items_processed']} items")
        
    except Exception as e:
        logger.error(f"Training error: {e}")
    finally:
        training_status["is_running"] = False

async def train_from_wikipedia():
    """ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Wikipedia"""
    try:
        topics = ["Python", "Artificial Intelligence", "Machine Learning", "Neural Network"]
        topic = topics[training_status["items_processed"] % len(topics)]
        
        page = wikipedia.page(topic, auto_suggest=True)
        content = page.content[:1000]
        
        add_knowledge(topic, content, f"wikipedia:{page.url}", confidence=0.9)
        logger.info(f"Added Wikipedia article: {topic}")
        
    except Exception as e:
        logger.error(f"Wikipedia training error: {e}")

async def train_from_github():
    """ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° GitHub (Ğ·Ğ°Ğ³Ğ»ÑƒÑˆĞºĞ°)"""
    logger.info("GitHub training (placeholder)")
    pass

async def train_from_habr():
    """ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Habr (Ğ·Ğ°Ğ³Ğ»ÑƒÑˆĞºĞ°)"""
    logger.info("Habr training (placeholder)")
    pass

@app.get("/training/status")
async def get_training_status():
    """Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ"""
    return training_status

@app.post("/training/stop")
async def stop_training():
    """ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ"""
    global training_status
    training_status["is_running"] = False
    return {"status": "stopped"}

# ==================== BACKUP ENDPOINTS ====================

@app.post("/backup/create")
async def create_backup_endpoint(request: Request):
    """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ±ÑĞºĞ°Ğ¿Ğ°"""
    try:
        data = await request.json()
        description = data.get("description", "Manual backup")
        
        result = backup_system.create_backup(description)
        
        return {
            "status": "success",
            "backup_id": result.get("commit_hash", "unknown"),
            "description": description,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Backup creation error: {e}")
        return {"error": str(e)}

@app.get("/backup/list")
async def list_backups():
    """Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ±ÑĞºĞ°Ğ¿Ğ¾Ğ²"""
    try:
        backups = backup_system.list_backups()
        return {"backups": backups, "count": len(backups)}
    except Exception as e:
        logger.error(f"Backup list error: {e}")
        return {"error": str(e)}

@app.post("/backup/restore")
async def restore_backup(request: Request):
    """Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ±ÑĞºĞ°Ğ¿Ğ°"""
    try:
        data = await request.json()
        backup_id = data.get("backup_id")
        
        if not backup_id:
            raise HTTPException(status_code=400, detail="backup_id required")
        
        result = backup_system.restore_backup(backup_id)
        
        return {"status": "success", "backup_id": backup_id, "message": "Backup restored"}
        
    except Exception as e:
        logger.error(f"Backup restore error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ==================== TEST ENDPOINTS ====================

@app.get("/test/run")
async def run_tests():
    """Ğ—Ğ°Ğ¿ÑƒÑĞº Ñ‚ĞµÑÑ‚Ğ¾Ğ²"""
    try:
        results = test_system.run_all_tests()
        return {
            "test_results": results,
            "total_tests": len(results),
            "passed": sum(1 for r in results if r.get("status") == "passed"),
            "failed": sum(1 for r in results if r.get("status") == "failed")
        }
    except Exception as e:
        logger.error(f"Test error: {e}")
        return {"error": str(e)}

# ==================== WEBSOCKET ====================

@app.websocket("/ws/chat")
async def websocket_chat(websocket: WebSocket):
    """WebSocket Ğ´Ğ»Ñ Ñ‡Ğ°Ñ‚Ğ°"""
    await websocket.accept()
    logger.info("WebSocket chat connected")
    
    try:
        while True:
            data = await websocket.receive_text()
            message = json.loads(data)
            
            query = message.get("query", "")
            model = message.get("model", "qwen2.5:7b-instruct-q5_K_M")
            
            if not query:
                await websocket.send_json({"error": "Query required"})
                continue
            
            full_response = ""
            async for chunk in stream_ollama(query, model=model):
                full_response += chunk
                await websocket.send_json({"type": "chunk", "content": chunk})
            
            await websocket.send_json({"type": "done", "full_response": full_response})
            
            save_conversation(query, full_response, [], model)
            
    except WebSocketDisconnect:
        logger.info("WebSocket chat disconnected")
    except Exception as e:
        logger.error(f"WebSocket error: {e}")
        try:
            await websocket.send_json({"error": str(e)})
        except:
            pass

@app.websocket("/ws/self-healing")
async def websocket_self_healing(websocket: WebSocket):
    """WebSocket Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ° self-healing"""
    await websocket.accept()
    logger.info("WebSocket self-healing connected")
    
    try:
        while True:
            status = {
                "timestamp": datetime.now().isoformat(),
                "status": "healthy",
                "last_check": "OK",
                "auto_fixes": 0,
                "system_load": psutil.cpu_percent()
            }
            await websocket.send_json(status)
            await asyncio.sleep(5)
            
    except WebSocketDisconnect:
        logger.info("WebSocket self-healing disconnected")
    except Exception as e:
        logger.error(f"WebSocket self-healing error: {e}")

# ==================== WEB SEARCH ENDPOINTS ====================

@app.get("/search/web")
# ==================== WEB SEARCH PRIORITY ====================
# 1. Google Custom Search (primary - has API key)
# 2. Fallback to DuckDuckGo if Google fails
# ==============================================================

async def web_search_endpoint(query: str, max_results: int = 5):
    """ĞŸĞ¾Ğ¸ÑĞº Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ"""
    try:
        results = search_web(query, max_results=max_results)
        return {"query": query, "results": results, "count": len(results)}
    except Exception as e:
        logger.error(f"Web search endpoint error: {e}")
        return {"error": str(e)}

@app.post("/search/scrape")
async def scrape_endpoint(request: Request):
    """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ URL"""
    try:
        data = await request.json()
        url = data.get("url")
        
        if not url:
            raise HTTPException(status_code=400, detail="URL required")
        
        text = scrape_url(url)
        
        return {"url": url, "text": text, "length": len(text)}
        
    except Exception as e:
        logger.error(f"Scrape error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== ĞĞ›Ğ˜ĞĞ¡Ğ« Ğ”Ğ›Ğ¯ Ğ¡ĞĞ’ĞœĞ•Ğ¡Ğ¢Ğ˜ĞœĞĞ¡Ğ¢Ğ˜ (Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞ˜Ğ• 404) ====================

@app.post("/ask")
async def ask_alias(request: Request):
    """ĞĞ»Ğ¸Ğ°Ñ Ğ´Ğ»Ñ /chat"""
    return await chat(request)

@app.get("/api/status")
async def status_alias():
    """ĞĞ»Ğ¸Ğ°Ñ Ğ´Ğ»Ñ /stats"""
    return await get_stats()

@app.get("/api/rag/stats")
async def rag_stats_alias():
    """ĞĞ»Ğ¸Ğ°Ñ Ğ´Ğ»Ñ /rag/stats"""
    return await rag_stats()

@app.get("/api/training/status")
async def training_status_alias():
    """ĞĞ»Ğ¸Ğ°Ñ Ğ´Ğ»Ñ /training/status"""
    return await get_training_status()

@app.post("/api/training/start")
async def training_start_alias(request: Request):
    """ĞĞ»Ğ¸Ğ°Ñ Ğ´Ğ»Ñ /training/start"""
    return await start_training(request)

@app.get("/api/model/stats")
async def model_stats_alias():
    """Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹"""
    try:
        conn = sqlite3.connect('/app/data/ii_agent.db')
        c = conn.cursor()
        c.execute("SELECT model_used, COUNT(*) as count FROM conversations GROUP BY model_used")
        stats = {row[0]: row[1] for row in c.fetchall()}
        conn.close()
        
        return {"model_usage": stats, "total": sum(stats.values())}
    except Exception as e:
        logger.error(f"Model stats error: {e}")
        return {"error": str(e)}

@app.get("/api/expert/requests")
async def expert_requests_alias():
    """Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğº ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼"""
    try:
        conn = sqlite3.connect('/app/data/ii_agent.db')
        c = conn.cursor()
        c.execute("""
            SELECT id, query, model_used, timestamp, rating 
            FROM conversations 
            WHERE model_used LIKE '%expert%' OR model_used LIKE '%32b%'
            ORDER BY timestamp DESC 
            LIMIT 50
        """)
        
        results = []
        for row in c.fetchall():
            results.append({
                "id": row[0],
                "query": row[1],
                "model": row[2],
                "timestamp": row[3],
                "rating": row[4],
                "status": "completed"
            })
        
        conn.close()
        return {"requests": results, "total": len(results)}
    except Exception as e:
        logger.error(f"Expert requests error: {e}")
        return {"error": str(e)}

# ==================== Ğ”ĞĞŸĞĞ›ĞĞ˜Ğ¢Ğ•Ğ›Ğ¬ĞĞ«Ğ• ENDPOINTS ====================

@app.post("/models/switch")
async def switch_model(request: Request):
    """ĞŸĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ"""
    try:
        data = await request.json()
        model = data.get("model")
        
        if not model:
            raise HTTPException(status_code=400, detail="Model name required")
        
        response = requests.get(f"{OLLAMA_API_URL}/api/tags", timeout=5)
        models = response.json().get("models", [])
        model_names = [m["name"] for m in models]
        
        if model not in model_names:
            raise HTTPException(status_code=404, detail=f"Model {model} not found")
        
        settings = await get_settings()
        settings["default_model"] = model
        
        config_file = Path("/app/data/settings.json")
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(settings, f, indent=2)
        
        return {"status": "success", "model": model, "message": f"Default model switched to {model}"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Model switch error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/conversation/rate")
async def rate_conversation(request: Request):
    """ĞÑ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°"""
    try:
        data = await request.json()
        conversation_id = data.get("conversation_id")
        rating = data.get("rating")
        
        if not conversation_id or rating is None:
            raise HTTPException(status_code=400, detail="conversation_id and rating required")
        
        conn = sqlite3.connect('/app/data/ii_agent.db')
        c = conn.cursor()
        c.execute("UPDATE conversations SET rating = ? WHERE id = ?", (rating, conversation_id))
        conn.commit()
        conn.close()
        
        return {"status": "success", "conversation_id": conversation_id, "rating": rating}
        
    except Exception as e:
        logger.error(f"Rating error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/knowledge/bulk-add")
async def bulk_add_knowledge(request: Request):
    """ĞœĞ°ÑÑĞ¾Ğ²Ğ¾Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹"""
    try:
        data = await request.json()
        items = data.get("items", [])
        
        if not items:
            raise HTTPException(status_code=400, detail="Items required")
        
        conn = sqlite3.connect('/app/data/ii_agent.db')
        c = conn.cursor()
        
        for item in items:
            topic = item.get("topic")
            content = item.get("content")
            source = item.get("source", "bulk_import")
            confidence = item.get("confidence", 0.7)
            
            if topic and content:
                c.execute(
                    "INSERT INTO knowledge_base (topic, content, source, confidence) VALUES (?, ?, ?, ?)",
                    (topic, content, source, confidence)
                )
        
        conn.commit()
        conn.close()
        
        return {"status": "success", "items_added": len(items)}
        
    except Exception as e:
        logger.error(f"Bulk add error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/system/disk-usage")
async def get_disk_usage():
    """Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ğ¿Ğ°Ğ¿ĞºĞ°Ğ¼"""
    try:
        data_dir = Path("/app/data")
        
        usage = {}
        for item in data_dir.iterdir():
            if item.is_dir():
                size = sum(f.stat().st_size for f in item.rglob('*') if f.is_file())
                usage[item.name] = {
                    "size_bytes": size,
                    "size_mb": round(size / (1024**2), 2),
                    "size_gb": round(size / (1024**3), 2)
                }
            elif item.is_file():
                size = item.stat().st_size
                usage[item.name] = {
                    "size_bytes": size,
                    "size_mb": round(size / (1024**2), 2)
                }
        
        return {
            "usage": usage,
            "total_mb": round(sum(u["size_bytes"] for u in usage.values()) / (1024**2), 2)
        }
        
    except Exception as e:
        logger.error(f"Disk usage error: {e}")
        return {"error": str(e)}

# ==================== STARTUP & SHUTDOWN ====================


# ==================== ENDPOINT Ğ”Ğ›Ğ¯ Ğ—ĞĞ“Ğ Ğ£Ğ—ĞšĞ˜ ĞœĞĞ”Ğ•Ğ›Ğ•Ğ™ (EN) ====================

@app.get("/models")
async def get_models_en():
    """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ollama"""
    try:
        response = requests.get("http://host.docker.internal:11434/api/tags", timeout=5)
        if response.status_code == 200:
            data = response.json()
            models_list = []
            
            for model in data.get("models", []):
                models_list.append({
                    "name": model.get("name", ""),
                    "size": model.get("size", 0),
                    "modified": model.get("modified_at", "")
                })
            
            return {"models": models_list, "count": len(models_list)}
        else:
            logger.warning(f"Ollama returned status {response.status_code}")
            return {"models": [], "count": 0}
            
    except Exception as e:
        logger.error(f"Error fetching models: {e}")
        return {"models": [], "count": 0, "error": str(e)}

@app.on_event("startup")
async def startup_event():
    """Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¿ÑƒÑĞºĞµ"""
    logger.info("=" * 50)
    logger.info("ğŸš€ II-Agent Pro v5.0 Starting...")
    logger.info("=" * 50)
    
    app.state.start_time = time.time()
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ollama
    try:
        response = requests.get(f"{OLLAMA_API_URL}/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            logger.info(f"âœ… Ollama connected: {len(models)} models available")
        else:
            logger.warning("âš ï¸ Ollama not responding")
    except Exception as e:
        logger.error(f"âŒ Ollama connection failed: {e}")
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ‘Ğ”
    try:
        conn = sqlite3.connect('/app/data/ii_agent.db')
        conn.close()
        logger.info("âœ… Database connected")
    except Exception as e:
        logger.error(f"âŒ Database error: {e}")
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ChromaDB
    if CHROMADB_AVAILABLE:
        logger.info("âœ… ChromaDB available")
    else:
        logger.warning("âš ï¸ ChromaDB not available")
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° GPU
    if HAS_GPU:
        logger.info("âœ… GPU detected")
    else:
        logger.warning("âš ï¸ No GPU detected")
    
    # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼
    try:
        backup_system
        logger.info("âœ… Backup system initialized")
    except Exception as e:
        logger.error(f"âŒ Backup system error: {e}")
    
    try:
        test_system
        logger.info("âœ… Test system initialized")
    except Exception as e:
        logger.error(f"âŒ Test system error: {e}")
    
    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° AGENT_INSTRUCTIONS.md
    try:
        instructions_path = Path("/app/AGENT_INSTRUCTIONS.md")
        if instructions_path.exists():
            with open(instructions_path, 'r', encoding='utf-8') as f:
                instructions = f.read()
            add_knowledge(
                "Agent Instructions",
                instructions,
                "agent_instructions",
                confidence=1.0
            )
            logger.info("âœ… Agent instructions loaded")
        else:
            logger.warning("âš ï¸ AGENT_INSTRUCTIONS.md not found")
    except Exception as e:
        logger.error(f"âŒ Agent instructions error: {e}")
    
    logger.info("=" * 50)
    logger.info("âœ… II-Agent Pro ready!")
    logger.info("=" * 50)

@app.on_event("shutdown")
async def shutdown_event():
    """Ğ—Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹"""
    logger.info("ğŸ›‘ II-Agent Pro shutting down...")
    
    if training_status["is_running"]:
        training_status["is_running"] = False
        logger.info("â¹ï¸ Training stopped")
    
    try:
        backup_system.create_backup("Automatic shutdown backup")
        logger.info("ğŸ’¾ Final backup created")
    except Exception as e:
        logger.error(f"âŒ Final backup error: {e}")
    
    logger.info("ğŸ‘‹ Goodbye!")

# ==================== Ğ—ĞĞŸĞ£Ğ¡Ğš Ğ¡Ğ•Ğ Ğ’Ğ•Ğ Ğ ====================



# ============ Ğ« Ğ¢Ğ« ============

@app.post("/api/rag/upload")
async def upload_file(file: UploadFile):
    """Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ñ„Ğ°Ğ¹Ğ»Ğ° Ğ² RAG"""
    try:
        content = await file.read()
        text = content.decode('utf-8')
        rag.add_document(text, source=f"upload:{file.filename}")
        return {"success": True, "message": f"Ğ°Ğ¹Ğ» {file.filename} Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½"}
    except Exception as e:
        return {"success": False, "error": str(e)}

@app.get("/api/rag/search")
async def search_rag(query: str, limit: int = 5):
    """Ğ¾Ğ¸ÑĞº Ğ² RAG"""
    try:
        results = rag.search(query, limit=limit)
        return {"results": results}
    except Exception as e:
        return {"error": str(e)}

@app.post("/api/rag/train")
async def train_from_url(url: str):
    """Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ URL"""
    try:
        import requests
        from bs4 import BeautifulSoup
        
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')
        text = soup.get_text()
        
        rag.add_document(text, source=f"url:{url}")
        return {"success": True, "message": f"Ğ±ÑƒÑ‡ĞµĞ½Ğ¾ Ñ {url}"}
    except Exception as e:
        return {"success": False, "error": str(e)}

@app.get("/api/models")
async def list_models():
    """Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹"""
    try:
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags")
        return response.json()
    except:
        return {"models": []}



# ==================== AI DEVELOPER API ====================
from ai_developer import AIDeveloper

ai_dev = AIDeveloper()

@app.post("/ai-dev/analyze")
async def ai_dev_analyze(request: Request):
    """Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ AI-Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ¼"""
    try:
        data = await request.json()
        task = data.get('task', '')
        
        if not task:
            return JSONResponse({'error': 'Task is required'}, status_code=400)
        
        provider = data.get("provider", "groq")
        analysis = ai_dev.analyze_task(task, provider)
        return JSONResponse({'success': True, 'analysis': analysis})
    except Exception as e:
        logger.error(f'AI Dev analyze error: {e}')
        return JSONResponse({'error': str(e)}, status_code=500)

@app.post("/ai-dev/backup")
async def ai_dev_backup(request: Request):
    """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ±ÑĞºĞ°Ğ¿Ğ° Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²"""
    try:
        data = await request.json()
        files = data.get('files', [])
        task = data.get('task', 'Manual backup')
        
        backup_id = ai_dev.create_backup(files, task)
        return JSONResponse({'success': True, 'backup_id': backup_id})
    except Exception as e:
        logger.error(f'AI Dev backup error: {e}')
        return JSONResponse({'error': str(e)}, status_code=500)

@app.post("/ai-dev/generate")
async def ai_dev_generate(request: Request):
    """ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ"""
    try:
        data = await request.json()
        task = data.get('task', '')
        file_path = data.get('file_path', '')
        current_code = data.get('current_code', '')
        
        solution = ai_dev.generate_solution(task, file_path, current_code)
        return JSONResponse({'success': True, 'solution': solution})
    except Exception as e:
        logger.error(f'AI Dev generate error: {e}')
        return JSONResponse({'error': str(e)}, status_code=500)

@app.post("/ai-dev/apply")
async def ai_dev_apply(request: Request):
    """Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ"""
    try:
        data = await request.json()
        file_path = data.get('file_path', '')
        new_code = data.get('new_code', '')
        
        success = ai_dev.apply_changes(file_path, new_code)
        return JSONResponse({'success': success})
    except Exception as e:
        logger.error(f'AI Dev apply error: {e}')
        return JSONResponse({'error': str(e)}, status_code=500)

@app.post("/ai-dev/rollback")
async def ai_dev_rollback(request: Request):
    """Ñ‚ĞºĞ°Ñ‚ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹"""
    try:
        data = await request.json()
        backup_id = data.get('backup_id', '')
        
        success = ai_dev.rollback(backup_id)
        return JSONResponse({'success': success})
    except Exception as e:
        logger.error(f'AI Dev rollback error: {e}')
        return JSONResponse({'error': str(e)}, status_code=500)

@app.get("/ai-dev/backups")
async def ai_dev_list_backups():
    """Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ±ÑĞºĞ°Ğ¿Ğ¾Ğ²"""
    try:
        backups = ai_dev.list_backups()
        return JSONResponse({'success': True, 'backups': backups})
    except Exception as e:
        logger.error(f'AI Dev list backups error: {e}')
        return JSONResponse({'error': str(e)}, status_code=500)

@app.get("/ai-dev/diff/{backup_id}")
async def ai_dev_get_diff(backup_id: str, file_path: str):
    """Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ diff"""
    try:
        diff = ai_dev.get_diff(file_path, backup_id)
        return JSONResponse({'success': True, 'diff': diff})
    except Exception as e:
        logger.error(f'AI Dev diff error: {e}')
        return JSONResponse({'error': str(e)}, status_code=500)


if __name__ == "__main__":
    import uvicorn
    
    Path("/app/data").mkdir(parents=True, exist_ok=True)
    Path("/app/logs").mkdir(parents=True, exist_ok=True)
    BACKUP_DIR.mkdir(parents=True, exist_ok=True)
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info",
        access_log=True,
        workers=1
    )

    global gigachat_client
    gigachat_client = GigaChatAPI()
    gigachat_client.connect()
