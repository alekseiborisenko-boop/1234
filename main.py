from dotenv import load_dotenv
import os
from pathlib import Path

import asyncio
import aiofiles
from gigachat_api import GigaChatAPI
import logging
import requests
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
import hashlib
import json
import time
import re
import os
import sys
from pathlib import Path
from datetime import datetime, timedelta
from fastapi import FastAPI, File, UploadFile, HTTPException, Request, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse, JSONResponse, StreamingResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
import GPUtil
import psutil
import subprocess
import yt_dlp
import sqlite3
import threading
from queue import Queue
import ast
import urllib.parse
from duckduckgo_search import DDGS  # Fallback only
from bs4 import BeautifulSoup
import wikipedia
import base64
import pytz

# ==================== NEW SYSTEMS ====================
from test_system import test_system
from backup_system import backup_system
from model_hierarchy import model_hierarchy

# ==================== LOGGING SETUP (FIXED!) ====================
log_dir = Path("/app/logs")
log_dir.mkdir(parents=True, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_dir / 'ii_agent.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)


# ==================== PERFORMANCE MODULES ====================
from cache_manager import CacheManager
from async_scraper import AsyncScraper

logger = logging.getLogger(__name__)

# Load environment variables
env_path = Path(__file__).parent / '.env'
if env_path.exists():
    load_dotenv(env_path)
    logger.info(f'‚úÖ Loaded .env from {env_path}')
else:
    logger.warning('‚ö†Ô∏è .env file not found')

# Initialize performance modules
cache_manager = CacheManager(db_path='data/cache.db', ttl=3600)
async_scraper = AsyncScraper(timeout=10, max_concurrent=5)
logger.info('? Performance modules initialized')


# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
MODEL_TIMEOUT = 300
MAX_FILE_SIZE = 10 * 1024 * 1024
CHROMA_DB_PATH = "/app/data/chroma_db"
BACKUP_DIR = Path("/app/data/backups")
BACKUP_DIR.mkdir(parents=True, exist_ok=True)
OLLAMA_API_URL = "http://host.docker.internal:11434"

# –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
try:
    gpus = GPUtil.getGPUs()
    HAS_GPU = len(gpus) > 0
    if HAS_GPU:
        logger.info(f"GPU detected: {gpus[0].name}, VRAM: {gpus[0].memoryTotal}MB")
    else:
        logger.warning("GPU not detected")
except:
    HAS_GPU = False
    logger.warning("GPU detection failed")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ ChromaDB
try:
    chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
    embedder = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
    CHROMADB_AVAILABLE = True
    logger.info("‚úÖ ChromaDB and SentenceTransformer initialized")
except Exception as e:
    CHROMADB_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è ChromaDB not available: {e}")

# ==================== –ë–ê–ó–ê –î–ê–ù–ù–´–• ====================
def init_db():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SQLite –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    conn = sqlite3.connect('/app/data/ii_agent.db')
    c = conn.cursor()
    
    c.execute('''CREATE TABLE IF NOT EXISTS conversations
                 (id INTEGER PRIMARY KEY AUTOINCREMENT,
                  user_id TEXT,
                  query TEXT,
                  response TEXT,
                  sources TEXT,
                  model_used TEXT,
                  timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                  rating INTEGER DEFAULT 0)''')
    
    c.execute('''CREATE TABLE IF NOT EXISTS knowledge_base
                 (id INTEGER PRIMARY KEY AUTOINCREMENT,
                  topic TEXT,
                  content TEXT,
                  source TEXT,
                  confidence REAL DEFAULT 0.5,
                  timestamp DATETIME DEFAULT CURRENT_TIMESTAMP)''')
    
    conn.commit()
    conn.close()

def save_conversation(query, response, sources, model_used, user_id="default"):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∏–∞–ª–æ–≥–∞ –≤ –±–∞–∑—É"""
    conn = sqlite3.connect('/app/data/ii_agent.db')
    c = conn.cursor()
    c.execute("INSERT INTO conversations (user_id, query, response, sources, model_used) VALUES (?, ?, ?, ?, ?)",
              (user_id, query, response, json.dumps(sources), model_used))
    conn.commit()
    conversation_id = c.lastrowid
    conn.close()
    return conversation_id

def get_db_cursor():
    """–ü–æ–ª—É—á–∏—Ç—å –∫—É—Ä—Å–æ—Ä –ë–î"""
    conn = sqlite3.connect('/app/data/ii_agent.db')
    return conn.cursor()

def add_knowledge(topic, content, source, confidence=0.8):
    """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –≤ –±–∞–∑—É"""
    conn = sqlite3.connect('/app/data/ii_agent.db')
    c = conn.cursor()
    c.execute("INSERT INTO knowledge_base (topic, content, source, confidence) VALUES (?, ?, ?, ?)",
              (topic, content, source, confidence))
    conn.commit()
    conn.close()

def search_knowledge_base(query):
    """–ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
    conn = sqlite3.connect('/app/data/ii_agent.db')
    c = conn.cursor()
    c.execute("SELECT topic, content, source, confidence FROM knowledge_base WHERE topic LIKE ? OR content LIKE ? ORDER BY confidence DESC LIMIT 5",
              (f"%{query}%", f"%{query}%"))
    results = c.fetchall()
    conn.close()
    return results

def get_conversation_history(user_id="default", limit=50):
    """–ü–æ–ª—É—á–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–æ–≤"""
    conn = sqlite3.connect('/app/data/ii_agent.db')
    c = conn.cursor()
    c.execute("SELECT query, response, timestamp FROM conversations WHERE user_id=? ORDER BY timestamp DESC LIMIT ?",
              (user_id, limit))
    history = c.fetchall()
    conn.close()
    return history

def get_db_stats():
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ë–î"""
    conn = sqlite3.connect('/app/data/ii_agent.db')
    c = conn.cursor()
    
    c.execute("SELECT COUNT(*) FROM conversations")
    total_conversations = c.fetchone()[0]
    
    c.execute("SELECT COUNT(*) FROM knowledge_base")
    total_knowledge = c.fetchone()[0]
    
    conn.close()
    
    return {
        "conversations": total_conversations,
        "knowledge_items": total_knowledge
    }

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î
init_db()
# ==================== WEB –ü–û–ò–°–ö ====================
def search_web(query, max_results=5):
    """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Google Custom Search Engine"""
    try:
        google_key = os.getenv('GOOGLE_API_KEY')
        google_cx = os.getenv('GOOGLE_CSE_CX')
        
        if not google_key or not google_cx:
            logger.error('‚ùå Google API keys not found in .env')
            return []
        
        url = 'https://www.googleapis.com/customsearch/v1'
        params = {
            'key': google_key,
            'cx': google_cx,
            'q': query,
            'num': max_results
        }
        
        # Check cache first
        cache_key = f"{query}_{max_results}"
        cached_results = cache_manager.get(cache_key, 'google_cse')
        if cached_results:
            logger.info(f'üéØ Cache HIT for Google CSE: {query[:50]}...')
            return cached_results

        logger.info(f'üîç Google CSE search: {query}')
        response = requests.get(url, params=params, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            results = []
            for item in data.get('items', []):
                results.append({
                    'title': item.get('title', ''),
                    'url': item.get('link', ''),
                    'snippet': item.get('snippet', '')
                })
            # Save to cache
            if results:
                cache_manager.set(cache_key, results, 'google_cse', ttl=3600)
                logger.info(f'üíæ Cached {len(results)} results')
            
            logger.info(f'‚úÖ Found {len(results)} results from Google CSE')
            return results
        else:
            logger.error(f'‚ùå Google CSE error: {response.status_code}')
            return []
            
    except Exception as e:
        logger.error(f'‚ùå Search error: {e}')
        return []


def get_weather(city='–£—Ñ–∞'):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ—á–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞ –ø–æ–≥–æ–¥—ã —á–µ—Ä–µ–∑ OpenWeatherMap API"""
    try:
        api_key = os.getenv('OPENWEATHER_API_KEY')
        if not api_key:
            logger.warning('OpenWeatherMap API key not found')
            return None
        
        url = f'http://api.openweathermap.org/data/2.5/weather'
        params = {
            'q': city,
            'appid': api_key,
            'units': 'metric',
            'lang': 'ru'
        }
        
        logger.info(f'üå§Ô∏è OpenWeatherMap API: {city}')
        response = requests.get(url, params=params, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            weather_info = {
                'temperature': data['main']['temp'],
                'feels_like': data['main']['feels_like'],
                'description': data['weather'][0]['description'],
                'humidity': data['main']['humidity'],
                'wind_speed': data['wind']['speed'],
                'city': data['name']
            }
            logger.info(f'‚úÖ Weather data received for {city}')
            return weather_info
        else:
            logger.error(f'‚ùå OpenWeatherMap error: {response.status_code}')
            return None
    except Exception as e:
        logger.error(f'‚ùå Weather API error: {e}')
        return None


def scrape_url(url, max_length=1500):
    """
    –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
    
    Args:
        url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1500)
    
    Returns:
        str: –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–ª–∏ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –ø—Ä–∏ –æ—à–∏–±–∫–µ
    """
    try:
        logger.info(f'üåê Scraping URL: {url}')
        
        response = requests.get(url, timeout=15, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8'
        })
        response.encoding = 'utf-8'
        
        if response.status_code != 200:
            logger.warning(f'‚ö†Ô∏è HTTP {response.status_code} for {url}')
            return ""
        
        soup = BeautifulSoup(response.content, 'lxml')
        
        # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        for script in soup(['script', 'style', 'nav', 'footer', 'aside', 'header', 'iframe', 'noscript', 'form']):
            script.decompose()
        
        # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã)
        content = None
        selectors = [
            'article', 
            'main', 
            '.article-body',
            '.article-content', 
            '.news-content', 
            '.post-content',
            '.entry-content',
            '[itemprop="articleBody"]'
        ]
        
        for selector in selectors:
            content = soup.select_one(selector)
            if content:
                logger.info(f'‚úÖ Found content with: {selector}')
                break
        
        if content:
            text = content.get_text(separator=' ', strip=True)
        else:
            # Fallback: –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
            paragraphs = soup.find_all('p')
            text = ' '.join([p.get_text().strip() for p in paragraphs if len(p.get_text().strip()) > 30])
            logger.info(f'‚úÖ Extracted {len(paragraphs)} paragraphs as fallback')
        
        # –û—á–∏—Å—Ç–∫–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = ' '.join(chunk for chunk in chunks if chunk)
        
        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –Ω—É–∂–Ω–æ–π –¥–ª–∏–Ω—ã
        if len(text) > max_length:
            text = text[:max_length] + '...'
        
        logger.info(f'‚úÖ Scraped {len(text)} chars from {url}')
        return text
        
    except requests.Timeout:
        logger.error(f'‚è±Ô∏è Timeout scraping {url}')
        return ""
    except Exception as e:
        logger.error(f'‚ùå Scraping error for {url}: {str(e)[:100]}')
        return ""


def detect_query_type(query):
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    
    Args:
        query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    
    Returns:
        str: –¢–∏–ø –∑–∞–ø—Ä–æ—Å–∞ ('weather', 'news', 'tutorial', 'general')
    """
    query_lower = query.lower()
    
    # –ü–æ–≥–æ–¥–∞
    weather_keywords = ['–ø–æ–≥–æ–¥–∞', '—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä', '–≥—Ä–∞–¥—É—Å', '–ø—Ä–æ–≥–Ω–æ–∑', '–∫–ª–∏–º–∞—Ç', 'weather', '–æ—Å–∞–¥–∫–∏', '–≤–µ—Ç–µ—Ä', '–¥–æ–∂–¥', '—Å–Ω–µ–≥']
    if any(kw in query_lower for kw in weather_keywords):
        logger.info('üå§Ô∏è Query type: WEATHER')
        return 'weather'
    
    # –ù–æ–≤–æ—Å—Ç–∏
    news_keywords = ['–Ω–æ–≤–æ—Å—Ç', '—Å–æ–±—ã—Ç–∏—è', '–ø—Ä–æ–∏–∑–æ—à–ª–æ', '—Å–ª—É—á–∏–ª–æ—Å—å', 'news', '—Å–µ–≥–æ–¥–Ω—è', '–≤—á–µ—Ä–∞', '–≥–ª–∞–≤–Ω—ã–µ']
    if any(kw in query_lower for kw in news_keywords):
        logger.info('üì∞ Query type: NEWS')
        return 'news'
    
    # –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏/–≥–∞–π–¥—ã
    tutorial_keywords = ['–∫–∞–∫ ', '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏', '—Å–ø–æ—Å–æ–±', '–º–µ—Ç–æ–¥', '–≥–∞–π–¥', 'tutorial', '–ø–æ—à–∞–≥', '–Ω–∞—É—á–∏', '–æ–±—ä—è—Å–Ω–∏ –∫–∞–∫']
    if any(kw in query_lower for kw in tutorial_keywords):
        logger.info('üìö Query type: TUTORIAL')
        return 'tutorial'
    
    # –û–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
    logger.info('üí¨ Query type: GENERAL')
    return 'general'

# ==================== OLLAMA –í–ó–ê–ò–ú–û–î–ï–ô–°–¢–í–ò–ï ====================


def filter_chinese_characters(text: str) -> str:
    if not text:
        return text
    # –∏–∞–ø–∞–∑–æ–Ω—ã –¥–ª—è –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤  –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
    chinese_ranges = [
        (0x4E00, 0x9FFF),   # —Å–Ω–æ–≤–Ω—ã–µ –∏–µ—Ä–æ–≥–ª–∏—Ñ—ã
        (0x3400, 0x4DBF),   # –∞—Å—à–∏—Ä–µ–Ω–∏–µ A
        (0x20000, 0x2A6DF), # –∞—Å—à–∏—Ä–µ–Ω–∏–µ B
        (0x3000, 0x303F),   # –∏—Ç–∞–π—Å–∫–∞—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è
        (0xFF00, 0xFFEF)    # –æ–ª–Ω–æ—à–∏—Ä–∏–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
    ]
    filtered_chars = []
    for char in text:
        char_code = ord(char)
        is_chinese = any(start <= char_code <= end for start, end in chinese_ranges)
        if not is_chinese:
            filtered_chars.append(char)
    return ''.join(filtered_chars).strip()



def query_ai(prompt, provider='ollama', model='qwen2.5:7b-instruct-q5_K_M', temperature=0.7):
    """????????????? ????? AI: Ollama, Groq ??? GigaChat"""
    try:
        if provider == 'ollama':
            return query_ollama(prompt, model, temperature)
        
        elif provider == 'groq':
            from cloud_models import GroqAPI
            groq = GroqAPI()
            return groq.chat(prompt, model='llama-3.3-70b-versatile')
        
        elif provider == 'gigachat':
            from cloud_models import GigaChatAPI
            gigachat = GigaChatAPI()
            return gigachat.chat(prompt, model='GigaChat-Pro')
        
        else:
            logger.error(f'Unknown provider: {provider}')
            return query_ollama(prompt, model, temperature)
    
    except Exception as e:
        logger.error(f'AI query error ({provider}): {e}')
        return query_ollama(prompt, model, temperature)


def query_ollama(prompt, model="qwen2.5:7b-instruct-q5_K_M", temperature=0.7):
    """–ó–∞–ø—Ä–æ—Å –∫ Ollama LLM"""
    try:
        logger.info(f"üîç Sending to Ollama: model={model}")
        
        response = requests.post(
            f"{OLLAMA_API_URL}/api/generate",
            json={
                "model": model,
                "prompt": prompt,
                "temperature": temperature,
                "stream": False
            },
            timeout=MODEL_TIMEOUT
        )
        
        if response.status_code == 200:
            return response.json().get("response", "")
        else:
            logger.error(f"Ollama API error: {response.status_code}")
            return None
            
    except Exception as e:
        logger.error(f"Ollama query error: {e}")
        return None

async def stream_ollama(prompt, model="qwen2.5:7b-instruct-q5_K_M", temperature=0.7):
    """–°—Ç—Ä–∏–º–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ –æ—Ç Ollama"""
    try:
        response = requests.post(
            f"{OLLAMA_API_URL}/api/generate",
            json={
                "model": model,
                "prompt": prompt,
                "temperature": temperature,
                "stream": True
            },
            stream=True,
            timeout=MODEL_TIMEOUT
        )
        
        for line in response.iter_lines():
            if line:
                data = json.loads(line)
                if "response" in data:
                    yield data["response"]
                    
    except Exception as e:
        logger.error(f"Ollama streaming error: {e}")
        yield f"Error: {str(e)}"

# ==================== –ê–î–ú–ò–ù–ö–ê API ====================

app = FastAPI(
    title="II-Agent Pro API",
    description="–õ–æ–∫–∞–ª—å–Ω—ã–π AI-–∞–≥–µ–Ω—Ç —Å —Å–∞–º–æ—Ä–∞–∑–≤–∏—Ç–∏–µ–º",
    version="5.0"
)


@app.get("/api/admin/models")
async def get_ollama_models():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π Ollama"""
    try:
        response = requests.get(f"{OLLAMA_API_URL}/api/tags", timeout=5)
        models_data = response.json()
        models = models_data.get("models", [])
        
        conn = sqlite3.connect('/app/data/ii_agent.db')
        c = conn.cursor()
        
        for model in models:
            model_name = model.get("name", "")
            c.execute("SELECT COUNT(*) FROM conversations WHERE model_used = ?", (model_name,))
            usage_count = c.fetchone()[0]
            model["usage_count"] = usage_count
            
        conn.close()
        
        return {"models": models, "status": "success"}
        
    except Exception as e:
        logger.error(f"Error getting models: {e}")
        return {"error": str(e), "models": []}

@app.post("/api/admin/models/pull")
async def pull_ollama_model(request: Request):
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å"""
    try:
        data = await request.json()
        model_name = data.get("model")
        
        if not model_name:
            raise HTTPException(status_code=400, detail="Model name required")
        
        response = requests.post(
            f"{OLLAMA_API_URL}/api/pull",
            json={"name": model_name},
            stream=True,
            timeout=3600
        )
        
        async def stream_progress():
            for line in response.iter_lines():
                if line:
                    yield f"data: {line.decode()}\n\n"
                    await asyncio.sleep(0.1)
        
        return StreamingResponse(stream_progress(), media_type="text/event-stream")
        
    except Exception as e:
        logger.error(f"Error pulling model: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.delete("/api/admin/models/{model_name}")
async def delete_ollama_model(model_name: str):
    """–£–¥–∞–ª–∏—Ç—å –º–æ–¥–µ–ª—å"""
    try:
        response = requests.delete(
            f"{OLLAMA_API_URL}/api/delete",
            json={"name": model_name},
            timeout=30
        )
        
        if response.status_code == 200:
            return {"status": "success", "message": f"Model {model_name} deleted"}
        else:
            return {"status": "error", "message": f"Failed to delete: {response.text}"}
            
    except Exception as e:
        logger.error(f"Error deleting model: {e}")
        return {"error": str(e)}

@app.get("/api/admin/system/stats")
async def get_system_stats():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–∏—Å—Ç–µ–º–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
    try:
        cpu_percent = psutil.cpu_percent(interval=1)
        cpu_count = psutil.cpu_count()
        
        ram = psutil.virtual_memory()
        ram_total = ram.total / (1024**3)
        ram_used = ram.used / (1024**3)
        ram_percent = ram.percent
        
        gpu_stats = []
        try:
            gpus = GPUtil.getGPUs()
            for gpu in gpus:
                gpu_stats.append({
                    "name": gpu.name,
                    "load": round(gpu.load * 100, 1),
                    "memory_used": gpu.memoryUsed,
                    "memory_total": gpu.memoryTotal,
                    "temperature": gpu.temperature
                })
        except:
            gpu_stats = []
        
        disk = psutil.disk_usage('/app/data')
        disk_total = disk.total / (1024**3)
        disk_used = disk.used / (1024**3)
        disk_percent = disk.percent
        
        return {
            "cpu": {"percent": round(cpu_percent, 1), "count": cpu_count},
            "ram": {"total_gb": round(ram_total, 2), "used_gb": round(ram_used, 2), "percent": round(ram_percent, 1)},
            "gpu": gpu_stats,
            "disk": {"total_gb": round(disk_total, 2), "used_gb": round(disk_used, 2), "percent": round(disk_percent, 1)},
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting system stats: {e}")
        return {"error": str(e)}

@app.get("/api/admin/logs")
async def get_logs(level: str = "INFO", limit: int = 100):
    """–ü–æ–ª—É—á–∏—Ç—å –ª–æ–≥–∏ —Å–∏—Å—Ç–µ–º—ã"""
    try:
        log_file = Path("/app/logs/ii_agent.log")
        
        if not log_file.exists():
            return {"logs": [], "message": "Log file not found"}
        
        with open(log_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        if level != "ALL":
            filtered = [line for line in lines if level in line]
        else:
            filtered = lines
        
        return {"logs": filtered[-limit:], "total": len(filtered), "level": level}
        
    except Exception as e:
        logger.error(f"Error reading logs: {e}")
        return {"error": str(e), "logs": []}

@app.post("/api/admin/settings")
async def update_settings(request: Request):
    """–û–±–Ω–æ–≤–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∏—Å—Ç–µ–º—ã"""
    try:
        settings = await request.json()
        config_file = Path("/app/data/settings.json")
        
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(settings, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Settings updated: {settings}")
        return {"status": "success", "settings": settings}
        
    except Exception as e:
        logger.error(f"Error updating settings: {e}")
        return {"error": str(e)}

@app.get("/api/admin/settings")
async def get_settings():
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏"""
    try:
        config_file = Path("/app/data/settings.json")
        
        if config_file.exists():
            with open(config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        default_settings = {
            "default_model": "qwen2.5:7b-instruct-q5_K_M",
            "temperature": 0.7,
            "max_tokens": 2000,
            "rag_top_k": 5,
            "enable_web_search": True,
            "enable_rag": True
        }
        
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(default_settings, f, indent=2)
        
        return default_settings
        
    except Exception as e:
        logger.error(f"Error loading settings: {e}")
        return {"error": str(e)}

# ==================== WEB SEARCH UNIFIED ====================
def web_search_unified(query: str, max_results: int = 5) -> list:
    """
    Unified web search with Google CSE primary and DuckDuckGo fallback.
    
    Args:
        query: Search query string
        max_results: Maximum number of results to return
        
    Returns:
        List of dicts with keys: url, title, snippet
    """
    results = []
    
    # Try Google Custom Search first
    google_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GOOGLE_CSE_API_KEY")
    google_cx = os.getenv("GOOGLE_CSE_CX") or os.getenv("GOOGLE_CSE_ID")
    
    if google_key and google_cx:
        try:
            logger.info(f"üîç Google CSE search: {query}")
            url = "https://www.googleapis.com/customsearch/v1"
            params = {
                "key": google_key,
                "cx": google_cx,
                "q": query,
                "num": max_results
            }
            response = requests.get(url, params=params, timeout=10)
            response.raise_for_status()
            data = response.json()
            
            for item in data.get("items", []):
                results.append({
                    "url": item.get("link", ""),
                    "title": item.get("title", ""),
                    "snippet": item.get("snippet", "")
                })
            
            if results:
                logger.info(f"‚úÖ Google CSE: found {len(results)} results")
                return results
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Google CSE failed: {e}, falling back to DuckDuckGo")
    
    # Fallback to DuckDuckGo
    try:
        logger.info(f"üîç DuckDuckGo search: {query}")
        ddg = DDGS()
        ddg_results = ddg.text(query, max_results=max_results)
        
        for item in ddg_results:
            results.append({
                "url": item.get("href", ""),
                "title": item.get("title", ""),
                "snippet": item.get("body", "")
            })
        
        logger.info(f"‚úÖ DuckDuckGo: found {len(results)} results")
        
    except Exception as e:
        logger.error(f"‚ùå DuckDuckGo failed: {e}")
    
    return results


# ==================== FASTAPI –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ====================

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== –û–°–ù–û–í–ù–´–ï ENDPOINTS ====================

@app.get("/")
async def root():
    """–ö–æ—Ä–Ω–µ–≤–æ–π endpoint"""
    return {
        "service": "II-Agent Pro API",
        "version": "5.0",
        "status": "running",
        "features": ["chat", "rag", "web_search", "training", "backup", "admin"]
    }

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–∏—Å—Ç–µ–º—ã"""
    try:
        ollama_response = requests.get(f"{OLLAMA_API_URL}/api/tags", timeout=5)
        ollama_status = "connected" if ollama_response.status_code == 200 else "disconnected"
        
        conn = sqlite3.connect('/app/data/ii_agent.db')
        conn.close()
        db_status = "healthy"
        
        chromadb_status = "available" if CHROMADB_AVAILABLE else "unavailable"
        
        return {
            "status": "healthy",
            "ollama": ollama_status,
            "database": db_status,
            "chromadb": chromadb_status,
            "gpu": "available" if HAS_GPU else "unavailable",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return {"status": "unhealthy", "error": str(e)}


@app.post("/ai-developer/generate")
async def generate_solution(request: dict):
    """–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏"""
    try:
        task = request.get('task', '')
        analysis = request.get('analysis', {})
        provider = request.get('provider', 'ollama')
        
        logger.info(f"üîÑ Generating solution with {provider}...")
        
        # –º–ø–æ—Ä—Ç–∏—Ä—É–µ–º ai_developer
        from ai_developer import AIdeveloper
        ai_dev = AIdev()
        
        # –µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ—à–µ–Ω–∏–µ
        solution = await ai_dev.generate_solution(task, analysis, provider)
        
        return {
            'success': True,
            'solution': solution
        }
    except Exception as e:
        logger.error(f"‚ùå Solution generation error: {e}")
        return {
            'success': False,
            'error': str(e)
        }

@app.get("/stats")
async def get_stats():
    """–û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã"""
    try:
        db_stats = get_db_stats()
        
        conn = sqlite3.connect('/app/data/ii_agent.db')
        c = conn.cursor()
        c.execute("SELECT model_used, COUNT(*) as count FROM conversations GROUP BY model_used")
        model_usage = {row[0]: row[1] for row in c.fetchall()}
        conn.close()
        
        uptime = time.time() - app.state.start_time if hasattr(app.state, 'start_time') else 0
        
        return {
            "total_conversations": db_stats["conversations"],
            "knowledge_items": db_stats["knowledge_items"],
            "model_usage": model_usage,
            "uptime_seconds": int(uptime),
            "chromadb_available": CHROMADB_AVAILABLE,
            "gpu_available": HAS_GPU
        }
    except Exception as e:
        logger.error(f"Stats error: {e}")
        return {"error": str(e)}

@app.get("/history")
async def get_history(limit: int = 50):
    """–ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–æ–≤"""
    try:
        history = get_conversation_history(limit=limit)
        return {
            "history": [{"query": h[0], "response": h[1], "timestamp": h[2]} for h in history],
            "count": len(history)
        }
    except Exception as e:
        logger.error(f"History error: {e}")
        return {"error": str(e)}

# ==================== –ß–ê–¢ ENDPOINTS ====================

@app.post("/chat")
async def chat(request: Request):
    """–û—Å–Ω–æ–≤–Ω–æ–π —á–∞—Ç endpoint"""
    try:
        data = await request.json()
        query = data.get("query", "")
        use_rag = data.get("use_rag", True)
        use_web = data.get("use_web", False)
        model = data.get("model", "qwen2.5:7b-instruct-q5_K_M")
        provider = data.get("provider", "ollama")
        
        if not query:
            raise HTTPException(status_code=400, detail="Query required")
        
        logger.info(f"Chat query: {query[:100]}...")
        
        context_parts = []
        sources = []
        
        if use_rag and CHROMADB_AVAILABLE:
            try:
                rag_results = search_knowledge_base(query)
                if rag_results:
                    context_parts.append("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:")
                    for topic, content, source, confidence in rag_results[:3]:
                        context_parts.append(f"- {content[:200]}")
                        sources.append({"type": "rag", "source": source, "confidence": confidence})
            except Exception as e:
                logger.error(f"RAG search error: {e}")
        
        if use_web:
            try:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞
                query_type = detect_query_type(query)
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∑–∞–ø—Ä–æ—Å –ø–æ–≥–æ–¥—ã
                weather_keywords = ['–ø–æ–≥–æ–¥–∞', '—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä', '–≥—Ä–∞–¥—É—Å', 'weather', '–ø—Ä–æ–≥–Ω–æ–∑', '–∫–ª–∏–º–∞—Ç', '–æ—Å–∞–¥–∫–∏', '–≤–µ—Ç–µ—Ä', '–¥–æ–∂–¥', '—Å–Ω–µ–≥']
                if any(keyword in query.lower() for keyword in weather_keywords):
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≥–æ—Ä–æ–¥ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –£—Ñ–∞)
                    city = '–£—Ñ–∞'
                    for word in ['–≤ ', '–¥–ª—è ', '–Ω–∞ ']:
                        if word in query.lower():
                            parts = query.lower().split(word)
                            if len(parts) > 1:
                                city = parts[1].split()[0].capitalize()
                                break
                    
                    weather_data = get_weather(city)
                    if weather_data:
                        context_parts.append(f"\n–¢–æ—á–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ –ø–æ–≥–æ–¥—ã –¥–ª—è {weather_data['city']}:")
                        context_parts.append(f"üå°Ô∏è –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: {weather_data['temperature']}¬∞C (–æ—â—É—â–∞–µ—Ç—Å—è –∫–∞–∫ {weather_data['feels_like']}¬∞C)")
                        context_parts.append(f"‚òÅÔ∏è –°–æ—Å—Ç–æ—è–Ω–∏–µ: {weather_data['description']}")
                        context_parts.append(f"üíß –í–ª–∞–∂–Ω–æ—Å—Ç—å: {weather_data['humidity']}%")
                        context_parts.append(f"üí® –í–µ—Ç–µ—Ä: {weather_data['wind_speed']} –º/—Å")
                        sources.append({"type": "weather", "city": city, "source": "OpenWeatherMap"})
                
                # –ü–æ–∏—Å–∫ –≤ Google CSE
                web_results = search_web(query, max_results=5)
                
                if web_results:
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞
                    if query_type == 'news':
                        # –î–õ–Ø –ù–û–í–û–°–¢–ï–ô: –ü–∞—Ä—Å–∏–º –ø–æ–ª–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã —Å—Ç–∞—Ç–µ–π
                        context_parts.append("\nüì∞ –ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:")
                        news_count = 0
                        
                        for result in web_results:
                            if news_count >= 3:  # –ú–∞–∫—Å–∏–º—É–º 3 –Ω–æ–≤–æ—Å—Ç–∏
                                break
                            
                            # –ü–∞—Ä—Å–∏–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
                            article_text = scrape_url(result['url'], max_length=1200)
                            
                            if article_text and len(article_text) > 200:
                                # –£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–∏–ª–∏ - –¥–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
                                context_parts.append(f"\nüîπ {result['title']}")
                                context_parts.append(f"{article_text}")
                                context_parts.append(f"[–ò—Å—Ç–æ—á–Ω–∏–∫: {result['url']}]")
                                sources.append({"type": "web", "title": result['title'], "url": result['url']})
                                news_count += 1
                            else:
                                # Fallback –Ω–∞ snippet –µ—Å–ª–∏ –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è
                                context_parts.append(f"\n- {result['title']}: {result['snippet'][:150]}")
                                sources.append({"type": "web", "title": result['title'], "url": result['url']})
                                news_count += 1
                                
                    elif query_type == 'tutorial':
                        # –î–õ–Ø –ò–ù–°–¢–†–£–ö–¶–ò–ô: –ü–∞—Ä—Å–∏–º —Å –±–æ–ª—å—à–æ–π –¥–ª–∏–Ω–æ–π
                        context_parts.append("\nüìö –ü–æ–¥—Ä–æ–±–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:")
                        
                        for result in web_results[:2]:  # –ú–∞–∫—Å–∏–º—É–º 2 –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                            article_text = scrape_url(result['url'], max_length=2000)
                            
                            if article_text and len(article_text) > 300:
                                context_parts.append(f"\nüìñ {result['title']}")
                                context_parts.append(f"{article_text}")
                                context_parts.append(f"[–ò—Å—Ç–æ—á–Ω–∏–∫: {result['url']}]")
                                sources.append({"type": "web", "title": result['title'], "url": result['url']})
                            else:
                                # Fallback –Ω–∞ snippet
                                context_parts.append(f"- {result['title']}: {result['snippet']}")
                                sources.append({"type": "web", "title": result['title'], "url": result['url']})
                    
                    else:
                        # –î–õ–Ø –û–ë–©–ò–• –í–û–ü–†–û–°–û–í: –ò—Å–ø–æ–ª—å–∑—É–µ–º snippets –∏–∑ Google
                        context_parts.append("\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞:")
                        for result in web_results[:3]:
                            context_parts.append(f"- {result['title']}: {result['snippet'][:150]} [–°—Å—ã–ª–∫–∞: {result['url']}]")
                            sources.append({"type": "web", "title": result['title'], "url": result['url']})
                            
            except Exception as e:
                logger.error(f"Web search error: {e}")
        
        context = "\n".join(context_parts) if context_parts else ""
        
        full_prompt = f"""–¢—ã - II-Agent Pro, –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏ –≤–æ –≤—Å–µ—Ö –æ–±–ª–∞—Å—Ç—è—Ö.

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
‚ö†Ô∏è –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û - –Ø–ó–´–ö –û–¢–í–ï–¢–ê (–ß–ò–¢–ê–ô –í–ù–ò–ú–ê–¢–ï–õ–¨–ù–û!):
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

üö´ –ê–ë–°–û–õ–Æ–¢–ù–û –ó–ê–ü–†–ï–©–ï–ù–û –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:
   - –ö–∏—Ç–∞–π—Å–∫–∏–π —è–∑—ã–∫: ‰∏≠Êñá, Ê±âËØ≠, Â§©Ê∞î, È¢ÑÊä•, Áõ∏ÂÖ≥, ‰ø°ÊÅØ, Á≠âÁ≠â
   - –ê–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫: English, weather, forecast, information, etc.
   - –õ—é–±—ã–µ –¥—Ä—É–≥–∏–µ —è–∑—ã–∫–∏!

‚úÖ –ï–î–ò–ù–°–¢–í–ï–ù–ù–´–ô –†–ê–ó–†–ï–®–Å–ù–ù–´–ô –Ø–ó–´–ö: –†–£–°–°–ö–ò–ô (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞)!

–°–¢–†–û–ì–ò–ï –ü–†–ê–í–ò–õ–ê (–ù–ê–†–£–®–ï–ù–ò–ï = –û–®–ò–ë–ö–ê):
1. –ü–∏—à–∏ –¢–û–õ–¨–ö–û —Ä—É—Å—Å–∫–∏–º–∏ –±—É–∫–≤–∞–º–∏: –∞, –±, –≤, –≥, –¥, –µ, —ë, –∂, –∑, –∏, –π, –∫, –ª, –º, –Ω, –æ, –ø, —Ä, —Å, —Ç, —É, —Ñ, —Ö, —Ü, —á, —à, —â, —ä, —ã, —å, —ç, —é, —è
2. –ó–ê–ü–†–ï–©–ï–ù–´ –∫–∏—Ç–∞–π—Å–∫–∏–µ –∏–µ—Ä–æ–≥–ª–∏—Ñ—ã: ‰∏≠, Êñá, Â§©, Ê∞î, È¢Ñ, Êä•, Áõ∏, ÂÖ≥, ‰ø°, ÊÅØ
3. –ó–ê–ü–†–ï–©–ï–ù–ê –ª–∞—Ç–∏–Ω–∏—Ü–∞ –≤ —Ç–µ–∫—Å—Ç–µ (—Ä–∞–∑—Ä–µ—à–µ–Ω–∞ —Ç–æ–ª—å–∫–æ –≤ URL)
4. –ï—Å–ª–∏ –∑–∞–º–µ—á–∞–µ—à—å —á—Ç–æ –Ω–∞—á–∏–Ω–∞–µ—à—å –ø–∏—Å–∞—Ç—å –Ω–∞ –¥—Ä—É–≥–æ–º —è–∑—ã–∫–µ - –û–°–¢–ê–ù–û–í–ò –°–ï–ë–Ø!
5. –ü–µ—Ä–µ—á–∏—Ç—ã–≤–∞–π –∫–∞–∂–¥–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø–µ—Ä–µ–¥ —Ç–µ–º –∫–∞–∫ –µ–≥–æ –Ω–∞–ø–∏—Å–∞—Ç—å
6. –≠—Ç–æ –ø—Ä–∞–≤–∏–ª–æ –¥–µ–π—Å—Ç–≤—É–µ—Ç –¥–æ –°–ê–ú–û–ì–û –ö–û–ù–¶–ê –æ—Ç–≤–µ—Ç–∞!
7. –ù–ï –ü–ò–®–ò –∫–∏—Ç–∞–π—Å–∫–∏–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏–ª–∏ –ø—Ä–∏–º–µ—á–∞–Ω–∏—è!

–ü–†–û–í–ï–†–ö–ê –ü–ï–†–ï–î –û–¢–ü–†–ê–í–ö–û–ô:
- –ï—Å—Ç—å –ª–∏ –≤ –º–æ—ë–º –æ—Ç–≤–µ—Ç–µ –∫–∏—Ç–∞–π—Å–∫–∏–µ –∏–µ—Ä–æ–≥–ª–∏—Ñ—ã? ‚Üí –£–î–ê–õ–ò –ò–•!
- –ï—Å—Ç—å –ª–∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ? ‚Üí –ü–ï–†–ï–í–ï–î–ò –ù–ê –†–£–°–°–ö–ò–ô!
- –í–µ—Å—å –æ—Ç–≤–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ? ‚Üí –¢–û–õ–¨–ö–û –¢–û–ì–î–ê –û–¢–ü–†–ê–í–õ–Ø–ô!

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üìã –°–¢–†–£–ö–¢–£–†–ê –ò –ö–ê–ß–ï–°–¢–í–û –û–¢–í–ï–¢–ê:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

1. –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
   ‚úì –ù–∞—á–∏–Ω–∞–π —Å –∫—Ä–∞—Ç–∫–æ–≥–æ —Ä–µ–∑—é–º–µ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
   ‚úì –î–∞–≤–∞–π —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–π, –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏
   ‚úì –ò—Å–ø–æ–ª—å–∑—É–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç: –∞–±–∑–∞—Ü—ã, —Å–ø–∏—Å–∫–∏, –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–∏
   ‚úì –î–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Ç–µ–º —Ä–∞–∑–±–∏–≤–∞–π –Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ä–∞–∑–¥–µ–ª—ã

2. –ö–ê–ß–ï–°–¢–í–û –ò–ù–§–û–†–ú–ê–¶–ò–ò:
   ‚úì –ü—Ä–∏–≤–æ–¥–∏ —Ç–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: —Ü–∏—Ñ—Ä—ã, –¥–∞—Ç—ã, –∏–º–µ–Ω–∞, –º–µ—Å—Ç–∞
   ‚úì –ò—Å–ø–æ–ª—å–∑—É–π –∞–∫—Ç—É–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
   ‚úì –ß—ë—Ç–∫–æ —Ä–∞–∑–¥–µ–ª—è–π —Ñ–∞–∫—Ç—ã –∏ –º–Ω–µ–Ω–∏—è
   ‚úì –ï—Å–ª–∏ –Ω–µ —É–≤–µ—Ä–µ–Ω - —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏, –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π

3. –ü–û–ì–û–î–ê –ò –†–ï–ê–õ–¨–ù–´–ï –î–ê–ù–ù–´–ï:
   ‚úì –î–ª—è –ø–æ–≥–æ–¥—ã –∏—Å–ø–æ–ª—å–∑—É–π —Ç–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ OpenWeatherMap API
   ‚úì –£–∫–∞–∑—ã–≤–∞–π: —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É, –æ—â—É—â–∞–µ—Ç—Å—è –∫–∞–∫, –≤–ª–∞–∂–Ω–æ—Å—Ç—å, –≤–µ—Ç–µ—Ä, –æ—Å–∞–¥–∫–∏
   ‚úì –î–∞–≤–∞–π –∫—Ä–∞—Ç–∫–∏–π –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–∏—Ö —É—Å–ª–æ–≤–∏–π

4. –ù–û–í–û–°–¢–ò –ò –°–û–ë–´–¢–ò–Ø:
   ‚úì –ü–µ—Ä–µ—á–∏—Å–ª—è–π —Å–æ–±—ã—Ç–∏—è –≤ —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–º –ø–æ—Ä—è–¥–∫–µ
   ‚úì –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–æ–±—ã—Ç–∏—è: –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
   ‚úì –£–∫–∞–∑—ã–≤–∞–π –∏—Å—Ç–æ—á–Ω–∏–∫–∏, –¥–∞—Ç—ã, –∫–ª—é—á–µ–≤—ã—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤

5. –¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –í–û–ü–†–û–°–´:
   ‚úì –î–∞–≤–∞–π –ø–æ—à–∞–≥–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≥–¥–µ –ø—Ä–∏–º–µ–Ω–∏–º–æ
   ‚úì –û–±—ä—è—Å–Ω—è–π —Å–ª–æ–∂–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º
   ‚úì –ü—Ä–∏–≤–æ–¥–∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã

6. –°–¢–ò–õ–¨ –û–ë–©–ï–ù–ò–Ø:
   ‚úì –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π, –Ω–æ –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π —Ç–æ–Ω
   ‚úì –ò–∑–±–µ–≥–∞–π –∏–∑–ª–∏—à–Ω–µ–π —Ñ–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ "–≤–æ–¥—ã"
   ‚úì –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–µ–Ω –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É
   ‚úì –ê–¥–∞–ø—Ç–∏—Ä—É–π—Å—è –ø–æ–¥ –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üìö –î–û–°–¢–£–ü–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

{context}

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
‚ùì –í–û–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

{query}

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üí¨ –¢–í–û–ô –û–¢–í–ï–¢ (–ü–û–ú–ù–ò: –¢–û–õ–¨–ö–û –†–£–°–°–ö–ò–ô –Ø–ó–´–ö –î–û –ö–û–ù–¶–ê!):
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
"""
        
        response = query_ai(full_prompt, provider=provider, model=model)
        if response:
            response = filter_chinese_characters(response)
        
        if response is None:
            raise HTTPException(status_code=500, detail="LLM query failed")
        
        conversation_id = save_conversation(query, response, sources, model)
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫–∏ –∫ –æ—Ç–≤–µ—Ç—É
        if sources:
            links_section = "\n\n---\nüìå **–ò—Å—Ç–æ—á–Ω–∏–∫–∏:**\n"
            for src in sources:
                if src.get('type') == 'web':
                    links_section += f"- [{src['title']}]({src['url']})\n"
                elif src.get('type') == 'weather':
                    links_section += f"- –ü–æ–≥–æ–¥–∞: OpenWeatherMap ({src['city']})\n"
            response += links_section
        
        return {
            "response": response,
            "sources": sources,
            "model": model,
            "conversation_id": conversation_id,
            "timestamp": datetime.now().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Chat error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ==================== RAG ENDPOINTS ====================

@app.post("/rag/add")
async def add_rag_item(request: Request):
    """–î–æ–±–∞–≤–∏—Ç—å —ç–ª–µ–º–µ–Ω—Ç –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
    try:
        data = await request.json()
        topic = data.get("topic", "")
        content = data.get("content", "")
        source = data.get("source", "manual")
        confidence = data.get("confidence", 0.8)
        
        if not topic or not content:
            raise HTTPException(status_code=400, detail="Topic and content required")
        
        add_knowledge(topic, content, source, confidence)
        
        return {"status": "success", "message": "Knowledge added", "topic": topic}
        
    except Exception as e:
        logger.error(f"RAG add error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/rag/search")
async def search_rag(query: str, limit: int = 5):
    """–ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
    try:
        results = search_knowledge_base(query)
        
        return {
            "query": query,
            "results": [
                {"topic": r[0], "content": r[1], "source": r[2], "confidence": r[3]}
                for r in results[:limit]
            ],
            "count": len(results)
        }
        
    except Exception as e:
        logger.error(f"RAG search error: {e}")
        return {"error": str(e)}

@app.get("/rag/stats")
async def rag_stats():
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ RAG"""
    try:
        conn = sqlite3.connect('/app/data/ii_agent.db')
        c = conn.cursor()
        
        c.execute("SELECT COUNT(*) FROM knowledge_base")
        total = c.fetchone()[0]
        
        c.execute("SELECT source, COUNT(*) as count FROM knowledge_base GROUP BY source")
        by_source = {row[0]: row[1] for row in c.fetchall()}
        
        c.execute("SELECT AVG(confidence) FROM knowledge_base")
        avg_confidence = c.fetchone()[0] or 0.0
        
        conn.close()
        
        return {
            "total_items": total,
            "by_source": by_source,
            "avg_confidence": round(avg_confidence, 2),
            "chromadb_available": CHROMADB_AVAILABLE
        }
        
    except Exception as e:
        logger.error(f"RAG stats error: {e}")
        return {"error": str(e)}

# ==================== TRAINING ENDPOINTS ====================

training_status = {
    "is_running": False,
    "progress": 0,
    "current_source": None,
    "items_processed": 0,
    "start_time": None
}

@app.post("/training/start")
async def start_training(request: Request):
    """–ó–∞–ø—É—Å–∫ –Ω–æ—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
    global training_status
    
    try:
        if training_status["is_running"]:
            return {"status": "error", "message": "Training already running"}
        
        data = await request.json()
        duration_hours = data.get("duration_hours", 4)
        cycles_per_hour = data.get("cycles_per_hour", 4)
        sources = data.get("sources", ["wikipedia", "github", "habr"])
        
        asyncio.create_task(run_training(duration_hours, cycles_per_hour, sources))
        
        return {
            "status": "started",
            "duration_hours": duration_hours,
            "cycles_per_hour": cycles_per_hour,
            "sources": sources
        }
        
    except Exception as e:
        logger.error(f"Training start error: {e}")
        return {"error": str(e)}

async def run_training(duration_hours, cycles_per_hour, sources):
    """–§–æ–Ω–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ"""
    global training_status
    
    training_status["is_running"] = True
    training_status["progress"] = 0
    training_status["start_time"] = datetime.now().isoformat()
    
    try:
        total_cycles = duration_hours * cycles_per_hour
        
        for cycle in range(total_cycles):
            source = sources[cycle % len(sources)]
            training_status["current_source"] = source
            training_status["progress"] = int((cycle / total_cycles) * 100)
            
            if source == "wikipedia":
                await train_from_wikipedia()
            elif source == "github":
                await train_from_github()
            elif source == "habr":
                await train_from_habr()
            
            training_status["items_processed"] += 1
            
            await asyncio.sleep(60 * 60 / cycles_per_hour)
        
        training_status["progress"] = 100
        logger.info(f"Training completed: {training_status['items_processed']} items")
        
    except Exception as e:
        logger.error(f"Training error: {e}")
    finally:
        training_status["is_running"] = False

async def train_from_wikipedia():
    """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ Wikipedia"""
    try:
        topics = ["Python", "Artificial Intelligence", "Machine Learning", "Neural Network"]
        topic = topics[training_status["items_processed"] % len(topics)]
        
        page = wikipedia.page(topic, auto_suggest=True)
        content = page.content[:1000]
        
        add_knowledge(topic, content, f"wikipedia:{page.url}", confidence=0.9)
        logger.info(f"Added Wikipedia article: {topic}")
        
    except Exception as e:
        logger.error(f"Wikipedia training error: {e}")

async def train_from_github():
    """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ GitHub (–∑–∞–≥–ª—É—à–∫–∞)"""
    logger.info("GitHub training (placeholder)")
    pass

async def train_from_habr():
    """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ Habr (–∑–∞–≥–ª—É—à–∫–∞)"""
    logger.info("Habr training (placeholder)")
    pass

@app.get("/training/status")
async def get_training_status():
    """–°—Ç–∞—Ç—É—Å –æ–±—É—á–µ–Ω–∏—è"""
    return training_status

@app.post("/training/stop")
async def stop_training():
    """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è"""
    global training_status
    training_status["is_running"] = False
    return {"status": "stopped"}

# ==================== BACKUP ENDPOINTS ====================

@app.post("/backup/create")
async def create_backup_endpoint(request: Request):
    """–°–æ–∑–¥–∞–Ω–∏–µ –±—ç–∫–∞–ø–∞"""
    try:
        data = await request.json()
        description = data.get("description", "Manual backup")
        
        result = backup_system.create_backup(description)
        
        return {
            "status": "success",
            "backup_id": result.get("commit_hash", "unknown"),
            "description": description,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Backup creation error: {e}")
        return {"error": str(e)}

@app.get("/backup/list")
async def list_backups():
    """–°–ø–∏—Å–æ–∫ –±—ç–∫–∞–ø–æ–≤"""
    try:
        backups = backup_system.list_backups()
        return {"backups": backups, "count": len(backups)}
    except Exception as e:
        logger.error(f"Backup list error: {e}")
        return {"error": str(e)}

@app.post("/backup/restore")
async def restore_backup(request: Request):
    """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –±—ç–∫–∞–ø–∞"""
    try:
        data = await request.json()
        backup_id = data.get("backup_id")
        
        if not backup_id:
            raise HTTPException(status_code=400, detail="backup_id required")
        
        result = backup_system.restore_backup(backup_id)
        
        return {"status": "success", "backup_id": backup_id, "message": "Backup restored"}
        
    except Exception as e:
        logger.error(f"Backup restore error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ==================== TEST ENDPOINTS ====================

@app.get("/test/run")
async def run_tests():
    """–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤"""
    try:
        results = test_system.run_all_tests()
        return {
            "test_results": results,
            "total_tests": len(results),
            "passed": sum(1 for r in results if r.get("status") == "passed"),
            "failed": sum(1 for r in results if r.get("status") == "failed")
        }
    except Exception as e:
        logger.error(f"Test error: {e}")
        return {"error": str(e)}

# ==================== WEBSOCKET ====================

@app.websocket("/ws/chat")
async def websocket_chat(websocket: WebSocket):
    """WebSocket –¥–ª—è —á–∞—Ç–∞"""
    await websocket.accept()
    logger.info("WebSocket chat connected")
    
    try:
        while True:
            data = await websocket.receive_text()
            message = json.loads(data)
            
            query = message.get("query", "")
            model = message.get("model", "qwen2.5:7b-instruct-q5_K_M")
            
            if not query:
                await websocket.send_json({"error": "Query required"})
                continue
            
            full_response = ""
            async for chunk in stream_ollama(query, model=model):
                full_response += chunk
                await websocket.send_json({"type": "chunk", "content": chunk})
            
            await websocket.send_json({"type": "done", "full_response": full_response})
            
            save_conversation(query, full_response, [], model)
            
    except WebSocketDisconnect:
        logger.info("WebSocket chat disconnected")
    except Exception as e:
        logger.error(f"WebSocket error: {e}")
        try:
            await websocket.send_json({"error": str(e)})
        except:
            pass

@app.websocket("/ws/self-healing")
async def websocket_self_healing(websocket: WebSocket):
    """WebSocket –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ self-healing"""
    await websocket.accept()
    logger.info("WebSocket self-healing connected")
    
    try:
        while True:
            status = {
                "timestamp": datetime.now().isoformat(),
                "status": "healthy",
                "last_check": "OK",
                "auto_fixes": 0,
                "system_load": psutil.cpu_percent()
            }
            await websocket.send_json(status)
            await asyncio.sleep(5)
            
    except WebSocketDisconnect:
        logger.info("WebSocket self-healing disconnected")
    except Exception as e:
        logger.error(f"WebSocket self-healing error: {e}")

# ==================== WEB SEARCH ENDPOINTS ====================

@app.get("/search/web")
# ==================== WEB SEARCH PRIORITY ====================
# 1. Google Custom Search (primary - has API key)
# 2. Fallback to DuckDuckGo if Google fails
# ==============================================================

async def web_search_endpoint(query: str, max_results: int = 5):
    """–ü–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ"""
    try:
        results = search_web(query, max_results=max_results)
        return {"query": query, "results": results, "count": len(results)}
    except Exception as e:
        logger.error(f"Web search endpoint error: {e}")
        return {"error": str(e)}

@app.post("/search/scrape")
async def scrape_endpoint(request: Request):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å URL"""
    try:
        data = await request.json()
        url = data.get("url")
        
        if not url:
            raise HTTPException(status_code=400, detail="URL required")
        
        text = scrape_url(url)
        
        return {"url": url, "text": text, "length": len(text)}
        
    except Exception as e:
        logger.error(f"Scrape error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== –ê–õ–ò–ê–°–´ –î–õ–Ø –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–ò (–ò–°–ü–†–ê–í–õ–ï–ù–ò–ï 404) ====================

@app.post("/ask")
async def ask_alias(request: Request):
    """–ê–ª–∏–∞—Å –¥–ª—è /chat"""
    return await chat(request)

@app.get("/api/status")
async def status_alias():
    """–ê–ª–∏–∞—Å –¥–ª—è /stats"""
    return await get_stats()

@app.get("/api/rag/stats")
async def rag_stats_alias():
    """–ê–ª–∏–∞—Å –¥–ª—è /rag/stats"""
    return await rag_stats()

@app.get("/api/training/status")
async def training_status_alias():
    """–ê–ª–∏–∞—Å –¥–ª—è /training/status"""
    return await get_training_status()

@app.post("/api/training/start")
async def training_start_alias(request: Request):
    """–ê–ª–∏–∞—Å –¥–ª—è /training/start"""
    return await start_training(request)

@app.get("/api/model/stats")
async def model_stats_alias():
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–æ–¥–µ–ª–µ–π"""
    try:
        conn = sqlite3.connect('/app/data/ii_agent.db')
        c = conn.cursor()
        c.execute("SELECT model_used, COUNT(*) as count FROM conversations GROUP BY model_used")
        stats = {row[0]: row[1] for row in c.fetchall()}
        conn.close()
        
        return {"model_usage": stats, "total": sum(stats.values())}
    except Exception as e:
        logger.error(f"Model stats error: {e}")
        return {"error": str(e)}

@app.get("/api/expert/requests")
async def expert_requests_alias():
    """–ò—Å—Ç–æ—Ä–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–º –º–æ–¥–µ–ª—è–º"""
    try:
        conn = sqlite3.connect('/app/data/ii_agent.db')
        c = conn.cursor()
        c.execute("""
            SELECT id, query, model_used, timestamp, rating 
            FROM conversations 
            WHERE model_used LIKE '%expert%' OR model_used LIKE '%32b%'
            ORDER BY timestamp DESC 
            LIMIT 50
        """)
        
        results = []
        for row in c.fetchall():
            results.append({
                "id": row[0],
                "query": row[1],
                "model": row[2],
                "timestamp": row[3],
                "rating": row[4],
                "status": "completed"
            })
        
        conn.close()
        return {"requests": results, "total": len(results)}
    except Exception as e:
        logger.error(f"Expert requests error: {e}")
        return {"error": str(e)}

# ==================== –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï ENDPOINTS ====================

@app.post("/models/switch")
async def switch_model(request: Request):
    """–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
    try:
        data = await request.json()
        model = data.get("model")
        
        if not model:
            raise HTTPException(status_code=400, detail="Model name required")
        
        response = requests.get(f"{OLLAMA_API_URL}/api/tags", timeout=5)
        models = response.json().get("models", [])
        model_names = [m["name"] for m in models]
        
        if model not in model_names:
            raise HTTPException(status_code=404, detail=f"Model {model} not found")
        
        settings = await get_settings()
        settings["default_model"] = model
        
        config_file = Path("/app/data/settings.json")
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(settings, f, indent=2)
        
        return {"status": "success", "model": model, "message": f"Default model switched to {model}"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Model switch error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/conversation/rate")
async def rate_conversation(request: Request):
    """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞"""
    try:
        data = await request.json()
        conversation_id = data.get("conversation_id")
        rating = data.get("rating")
        
        if not conversation_id or rating is None:
            raise HTTPException(status_code=400, detail="conversation_id and rating required")
        
        conn = sqlite3.connect('/app/data/ii_agent.db')
        c = conn.cursor()
        c.execute("UPDATE conversations SET rating = ? WHERE id = ?", (rating, conversation_id))
        conn.commit()
        conn.close()
        
        return {"status": "success", "conversation_id": conversation_id, "rating": rating}
        
    except Exception as e:
        logger.error(f"Rating error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/knowledge/bulk-add")
async def bulk_add_knowledge(request: Request):
    """–ú–∞—Å—Å–æ–≤–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π"""
    try:
        data = await request.json()
        items = data.get("items", [])
        
        if not items:
            raise HTTPException(status_code=400, detail="Items required")
        
        conn = sqlite3.connect('/app/data/ii_agent.db')
        c = conn.cursor()
        
        for item in items:
            topic = item.get("topic")
            content = item.get("content")
            source = item.get("source", "bulk_import")
            confidence = item.get("confidence", 0.7)
            
            if topic and content:
                c.execute(
                    "INSERT INTO knowledge_base (topic, content, source, confidence) VALUES (?, ?, ?, ?)",
                    (topic, content, source, confidence)
                )
        
        conn.commit()
        conn.close()
        
        return {"status": "success", "items_added": len(items)}
        
    except Exception as e:
        logger.error(f"Bulk add error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/system/disk-usage")
async def get_disk_usage():
    """–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∏—Å–∫–∞ –ø–æ –ø–∞–ø–∫–∞–º"""
    try:
        data_dir = Path("/app/data")
        
        usage = {}
        for item in data_dir.iterdir():
            if item.is_dir():
                size = sum(f.stat().st_size for f in item.rglob('*') if f.is_file())
                usage[item.name] = {
                    "size_bytes": size,
                    "size_mb": round(size / (1024**2), 2),
                    "size_gb": round(size / (1024**3), 2)
                }
            elif item.is_file():
                size = item.stat().st_size
                usage[item.name] = {
                    "size_bytes": size,
                    "size_mb": round(size / (1024**2), 2)
                }
        
        return {
            "usage": usage,
            "total_mb": round(sum(u["size_bytes"] for u in usage.values()) / (1024**2), 2)
        }
        
    except Exception as e:
        logger.error(f"Disk usage error: {e}")
        return {"error": str(e)}

# ==================== STARTUP & SHUTDOWN ====================


# ==================== ENDPOINT –î–õ–Ø –ó–ê–ì–†–£–ó–ö–ò –ú–û–î–ï–õ–ï–ô (EN) ====================

@app.get("/models")
async def get_models_en():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Ollama"""
    try:
        response = requests.get("http://host.docker.internal:11434/api/tags", timeout=5)
        if response.status_code == 200:
            data = response.json()
            models_list = []
            
            for model in data.get("models", []):
                models_list.append({
                    "name": model.get("name", ""),
                    "size": model.get("size", 0),
                    "modified": model.get("modified_at", "")
                })
            
            return {"models": models_list, "count": len(models_list)}
        else:
            logger.warning(f"Ollama returned status {response.status_code}")
            return {"models": [], "count": 0}
            
    except Exception as e:
        logger.error(f"Error fetching models: {e}")
        return {"models": [], "count": 0, "error": str(e)}

@app.on_event("startup")
async def startup_event():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ"""
    logger.info("=" * 50)
    logger.info("üöÄ II-Agent Pro v5.0 Starting...")
    logger.info("=" * 50)
    
    app.state.start_time = time.time()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ Ollama
    try:
        response = requests.get(f"{OLLAMA_API_URL}/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            logger.info(f"‚úÖ Ollama connected: {len(models)} models available")
        else:
            logger.warning("‚ö†Ô∏è Ollama not responding")
    except Exception as e:
        logger.error(f"‚ùå Ollama connection failed: {e}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ë–î
    try:
        conn = sqlite3.connect('/app/data/ii_agent.db')
        conn.close()
        logger.info("‚úÖ Database connected")
    except Exception as e:
        logger.error(f"‚ùå Database error: {e}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ ChromaDB
    if CHROMADB_AVAILABLE:
        logger.info("‚úÖ ChromaDB available")
    else:
        logger.warning("‚ö†Ô∏è ChromaDB not available")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
    if HAS_GPU:
        logger.info("‚úÖ GPU detected")
    else:
        logger.warning("‚ö†Ô∏è No GPU detected")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º
    try:
        backup_system
        logger.info("‚úÖ Backup system initialized")
    except Exception as e:
        logger.error(f"‚ùå Backup system error: {e}")
    
    try:
        test_system
        logger.info("‚úÖ Test system initialized")
    except Exception as e:
        logger.error(f"‚ùå Test system error: {e}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ AGENT_INSTRUCTIONS.md
    try:
        instructions_path = Path("/app/AGENT_INSTRUCTIONS.md")
        if instructions_path.exists():
            with open(instructions_path, 'r', encoding='utf-8') as f:
                instructions = f.read()
            add_knowledge(
                "Agent Instructions",
                instructions,
                "agent_instructions",
                confidence=1.0
            )
            logger.info("‚úÖ Agent instructions loaded")
        else:
            logger.warning("‚ö†Ô∏è AGENT_INSTRUCTIONS.md not found")
    except Exception as e:
        logger.error(f"‚ùå Agent instructions error: {e}")
    
    logger.info("=" * 50)
    logger.info("‚úÖ II-Agent Pro ready!")
    logger.info("=" * 50)

@app.on_event("shutdown")
async def shutdown_event():
    """–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã"""
    logger.info("üõë II-Agent Pro shutting down...")
    
    if training_status["is_running"]:
        training_status["is_running"] = False
        logger.info("‚èπÔ∏è Training stopped")
    
    try:
        backup_system.create_backup("Automatic shutdown backup")
        logger.info("üíæ Final backup created")
    except Exception as e:
        logger.error(f"‚ùå Final backup error: {e}")
    
    logger.info("üëã Goodbye!")

# ==================== –ó–ê–ü–£–°–ö –°–ï–†–í–ï–†–ê ====================



# ============ –´ –¢–´ ============

@app.post("/api/rag/upload")
async def upload_file(file: UploadFile):
    """–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ –≤ RAG"""
    try:
        content = await file.read()
        text = content.decode('utf-8')
        rag.add_document(text, source=f"upload:{file.filename}")
        return {"success": True, "message": f"–∞–π–ª {file.filename} –∑–∞–≥—Ä—É–∂–µ–Ω"}
    except Exception as e:
        return {"success": False, "error": str(e)}

@app.get("/api/rag/search")
async def search_rag(query: str, limit: int = 5):
    """–æ–∏—Å–∫ –≤ RAG"""
    try:
        results = rag.search(query, limit=limit)
        return {"results": results}
    except Exception as e:
        return {"error": str(e)}

@app.post("/api/rag/train")
async def train_from_url(url: str):
    """–±—É—á–µ–Ω–∏–µ —Å URL"""
    try:
        import requests
        from bs4 import BeautifulSoup
        
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')
        text = soup.get_text()
        
        rag.add_document(text, source=f"url:{url}")
        return {"success": True, "message": f"–±—É—á–µ–Ω–æ —Å {url}"}
    except Exception as e:
        return {"success": False, "error": str(e)}

@app.get("/api/models")
async def list_models():
    """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    try:
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags")
        return response.json()
    except:
        return {"models": []}



# ==================== AI DEVELOPER API ====================
from ai_developer import AIDeveloper

ai_dev = AIDeveloper()

@app.post("/ai-dev/analyze")
async def ai_dev_analyze(request: Request):
    """–Ω–∞–ª–∏–∑ –∑–∞–¥–∞—á–∏ AI-—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–º"""
    try:
        data = await request.json()
        task = data.get('task', '')
        
        if not task:
            return JSONResponse({'error': 'Task is required'}, status_code=400)
        
        provider = data.get("provider", "groq")
        analysis = ai_dev.analyze_task(task, provider)
        return JSONResponse({'success': True, 'analysis': analysis})
    except Exception as e:
        logger.error(f'AI Dev analyze error: {e}')
        return JSONResponse({'error': str(e)}, status_code=500)

@app.post("/ai-dev/backup")
async def ai_dev_backup(request: Request):
    """–°–æ–∑–¥–∞–Ω–∏–µ –±—ç–∫–∞–ø–∞ —Ñ–∞–π–ª–æ–≤"""
    try:
        data = await request.json()
        files = data.get('files', [])
        task = data.get('task', 'Manual backup')
        
        backup_id = ai_dev.create_backup(files, task)
        return JSONResponse({'success': True, 'backup_id': backup_id})
    except Exception as e:
        logger.error(f'AI Dev backup error: {e}')
        return JSONResponse({'error': str(e)}, status_code=500)

@app.post("/ai-dev/generate")
async def ai_dev_generate(request: Request):
    """–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ—à–µ–Ω–∏—è"""
    try:
        data = await request.json()
        task = data.get('task', '')
        file_path = data.get('file_path', '')
        current_code = data.get('current_code', '')
        
        solution = ai_dev.generate_solution(task, file_path, current_code)
        return JSONResponse({'success': True, 'solution': solution})
    except Exception as e:
        logger.error(f'AI Dev generate error: {e}')
        return JSONResponse({'error': str(e)}, status_code=500)

@app.post("/ai-dev/apply")
async def ai_dev_apply(request: Request):
    """—Ä–∏–º–µ–Ω–∏—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è"""
    try:
        data = await request.json()
        file_path = data.get('file_path', '')
        new_code = data.get('new_code', '')
        
        success = ai_dev.apply_changes(file_path, new_code)
        return JSONResponse({'success': success})
    except Exception as e:
        logger.error(f'AI Dev apply error: {e}')
        return JSONResponse({'error': str(e)}, status_code=500)

@app.post("/ai-dev/rollback")
async def ai_dev_rollback(request: Request):
    """—Ç–∫–∞—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
    try:
        data = await request.json()
        backup_id = data.get('backup_id', '')
        
        success = ai_dev.rollback(backup_id)
        return JSONResponse({'success': success})
    except Exception as e:
        logger.error(f'AI Dev rollback error: {e}')
        return JSONResponse({'error': str(e)}, status_code=500)

@app.get("/ai-dev/backups")
async def ai_dev_list_backups():
    """–°–ø–∏—Å–æ–∫ –±—ç–∫–∞–ø–æ–≤"""
    try:
        backups = ai_dev.list_backups()
        return JSONResponse({'success': True, 'backups': backups})
    except Exception as e:
        logger.error(f'AI Dev list backups error: {e}')
        return JSONResponse({'error': str(e)}, status_code=500)

@app.get("/ai-dev/diff/{backup_id}")
async def ai_dev_get_diff(backup_id: str, file_path: str):
    """–æ–ª—É—á–∏—Ç—å diff"""
    try:
        diff = ai_dev.get_diff(file_path, backup_id)
        return JSONResponse({'success': True, 'diff': diff})
    except Exception as e:
        logger.error(f'AI Dev diff error: {e}')
        return JSONResponse({'error': str(e)}, status_code=500)


if __name__ == "__main__":
    import uvicorn
    
    Path("/app/data").mkdir(parents=True, exist_ok=True)
    Path("/app/logs").mkdir(parents=True, exist_ok=True)
    BACKUP_DIR.mkdir(parents=True, exist_ok=True)
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info",
        access_log=True,
        workers=1
    )

    global gigachat_client
    gigachat_client = GigaChatAPI()
    gigachat_client.connect()
